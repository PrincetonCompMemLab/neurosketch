{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "from sklearn import linear_model\n",
    "from brainiak.searchlight.searchlight import Searchlight\n",
    "from brainiak.searchlight.searchlight import Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = ''\n",
    "\n",
    "data_dir = '/jukebox/ntb/projects/sketchloop02/data'\n",
    "SAVE_PATH = '/jukebox/ntb/projects/sketchloop02/data/searchlight_output'\n",
    "curr_dir = '/jukebox/ntb/projects/sketchloop02/prototype/link'\n",
    "roi_dir = os.path.join(data_dir, 'copes/roi')\n",
    "cope_dir = os.path.join(data_dir, 'copes/recog/objectGLM')\n",
    "sub_dirs = sorted(os.listdir(roi_dir))\n",
    "sub_dirs = [each[:7] for each in sub_dirs]\n",
    "\n",
    "## root paths\n",
    "os.path.abspath(os.path.join(curr_dir,'..','..'))\n",
    "proj_dir = os.path.abspath(os.path.join(curr_dir,'..','..')) ## use relative paths\n",
    "feat_dir = os.path.abspath(os.path.join(proj_dir,'data/features')) ## use relative paths 'D:\\\\data'\n",
    "\n",
    "# null mask\n",
    "phases = ['34']\n",
    "null_mask = np.ones((88, 128, 128))\n",
    "normalize_on = True\n",
    "logged = False\n",
    "iv = 'Unnamed: 0'\n",
    "affine = np.array([[-1.996683955192566, -0.026332620531320572, -0.11206881701946259, 91.78023529052734],\n",
    "                   [-0.026291240006685257, 1.9998265504837036, -0.0014756681630387902, -125.46440124511719],\n",
    "                   [-0.11207851767539978, 7.630718279472148e-09, 1.9968571662902832, -120.91204833984375],\n",
    "                   [0.0, 0.0, 0.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_draw_metadata(subject):\n",
    "    metadata = pd.read_csv(\n",
    "        '/jukebox/ntb/projects/sketchloop02/data/feature_matrices_and_metadata/metadata_' +\n",
    "        subject + '_drawing.csv')\n",
    "    return metadata\n",
    "\n",
    "# z-score normalization to de-mean & standardize variances within-voxel\n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define and run searchlight\n",
    "How searchlight works with brainiak:\n",
    "\n",
    "1. Initiate a searchlight object, articulating certain parameters (e.g., searchlight shape, radius, max edge length, in voxels, of the 3D block(?)).\n",
    "2. Distribute data to be searched to the searchlight object, sorting between MPI ranks (idk what MPI ranks are tbh).\n",
    "3. Broadcast data, i.e., define other variables to be available for each execution of the searchlight function.\n",
    "4. Run the searchlight, this time articulating as a parameter the function to be applied at each searchlight location.\n",
    "\n",
    "We want to perform searchlight again, but this time the objective is to perform a version of helpers.make_drawing_predictions on each searchlight mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepostclf(subject_data, mask, sl_rad, bcast_var):\n",
    "    t1, t2 = trained_objs\n",
    "    subject_data = subject_data[0].transpose((3, 0, 1, 2))\n",
    "    \n",
    "    # format the train/test split\n",
    "    x_A = subject_data[:80,:].reshape(80, -1)\n",
    "    x_B = subject_data[80:, :].reshape(80, -1)\n",
    "    y_A = RM.label.values[:80]\n",
    "    y_B = RM.label.values[80:]\n",
    "    \n",
    "    # normalize if we want\n",
    "    if normalize_on:\n",
    "        x_A = normalize(x_A)\n",
    "        x_B = normalize(x_B)\n",
    "        \n",
    "    # dual classifiers\n",
    "    clfA = linear_model.LogisticRegression(\n",
    "        penalty='l2',C=1).fit(x_A, y_A)\n",
    "    clfB = linear_model.LogisticRegression(\n",
    "        penalty='l2',C=1).fit(x_B, y_B)\n",
    "    \n",
    "    ## add prediction probabilities to metadata matrix\n",
    "    ## must sort so that trained are first, and control is last\n",
    "    ## also save out new columns in the same order\n",
    "    _ordering = np.argsort(np.hstack((trained_objs,control_objs))) ## e.g., [chair table bench bed] ==> [3 2 0 1]\n",
    "    ordering = np.argsort(_ordering) ## get indices that sort from alphabetical to (trained_objs, control_objs)\n",
    "    probsA = (np.log(clfA.predict_proba(x_B)) if logged else clfA.predict_proba(x_B))\n",
    "    probsB = (np.log(clfB.predict_proba(x_A)) if logged else clfB.predict_proba(x_A))\n",
    "    probs = np.concatenate((probsA, probsB), axis=0)\n",
    "    \n",
    "    out = probs[:,ordering]\n",
    "    RM['t1_prob'] = out[:,0]\n",
    "    RM['t2_prob'] = out[:,1]\n",
    "    RM['c1_prob'] = out[:,2]\n",
    "    RM['c2_prob'] = out[:,3]\n",
    "    RM['t1_max'] = pd.Series(np.logical_and(np.logical_and(out[:,0] > out[:,1], out[:,0] > out[:,2]), out[:,0] > out[:,3]))\n",
    "    RM['t2_max'] = pd.Series(np.logical_and(np.logical_and(out[:,1] > out[:,0], out[:,1] > out[:,2]), out[:,1] > out[:,3]))\n",
    "    \n",
    "    # return proportion of times target > others\n",
    "    if RM[RM.label==t2].shape[0] + RM[RM.label==t1].shape[0]:\n",
    "        return (RM[(RM.label==t2) & (RM['t2_max'])].shape[0] + RM[(RM.label==t1) & (RM['t1_max'])].shape[0])/(\n",
    "            RM[RM.label==t2].shape[0] + RM[RM.label==t1].shape[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that sets up and organizes searchlight over a set of subjects\n",
    "def searchlight_over_each(subjects, sl_rad, phases):\n",
    "    global RM, DM, trained_objs, control_objs, subject\n",
    "    \n",
    "    for subject in tqdm(subjects):\n",
    "        print(subject)\n",
    "        \n",
    "        # set up searchlight object\n",
    "        sl = Searchlight(sl_rad=sl_rad, shape=Cube)\n",
    "        \n",
    "        # arrange data to be distributed to searchlight\n",
    "        # list of 4D not 3D arrays\n",
    "        subject_data = []\n",
    "        for phase in phases:\n",
    "            this_file = '{}/recog/{}_{}_featurematrix.npy'.format(\n",
    "                feat_dir, subject, phase)\n",
    "            subject_data.append(np.load(this_file).transpose((2, 3, 1, 0)))\n",
    "        subject_data = np.stack(subject_data, axis=0)\n",
    "        \n",
    "        # load metadata\n",
    "        this_file = \"{}/recog/metadata_{}_{}.csv\".format(feat_dir, subject, phase)\n",
    "        RM, DM = pd.read_csv(this_file), load_draw_metadata(subject)\n",
    "        trained_objs = np.unique(DM.label.values)\n",
    "        control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        \n",
    "        # distribute and broadcast needed data to searchlight\n",
    "        sl.distribute(subject_data, null_mask)\n",
    "        sl.broadcast(None)\n",
    "        \n",
    "        # run searchlight\n",
    "        subject_outputs = np.array(sl.run_searchlight(prepostclf))\n",
    "        print(np.shape(subject_outputs.astype(np.float32)))\n",
    "        img = nib.Nifti1Image(subject_outputs.astype(np.float32), affine)\n",
    "        \n",
    "        # store output for this subject\n",
    "        if not os.path.exists(SAVE_PATH):\n",
    "            os.makedirs(SAVE_PATH)\n",
    "        \n",
    "        localizer_tmap_filename = os.path.join(SAVE_PATH, subject + 'prepostacc_{}_searchlight.nii.gz'.format(phase))\n",
    "        nib.save(img, localizer_tmap_filename)\n",
    "        \n",
    "# run searchlight over all subjects and print time to compute results\n",
    "searchlight_over_each(sub_dirs, 3, phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of Maps into Standard Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Jupyter Magic for executing terminal commands, call FSL's flirt function on each searchlight map to convert them into standard space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for sub in sub_dirs:\n",
    "    print(sub)\n",
    "    flirtin = os.path.join(SAVE_PATH, sub+'prepostacc_56_searchlight.nii.gz')\n",
    "    flirtout = os.path.join(SAVE_PATH, sub+'_standard_prepostacc_56_searchlight.nii.gz')\n",
    "    flirtref = '/jukebox/ntb/projects/sketchloop02/searchlight_outputs/MNI152_T1_2mm_brain.nii.gz'\n",
    "    flirtinit = '/jukebox/ntb/users/jwammes/sketchloop/subjects/' + sub + '_neurosketch/analysis/firstlevel/draw_run_1.feat/reg/highres2standard.mat'\n",
    "    !module load fsl; flirt -in \"$flirtin\" -out \"$flirtout\" -ref \"$flirtref\" -applyxfm -init \"$flirtinit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
