{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "#import os\n",
    "#from glob import glob\n",
    "\n",
    "#from PIL import Image\n",
    "#from copy import deepcopy\n",
    "\n",
    "from sklearn import linear_model#, datasets, neighbors\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn import svm\n",
    "\n",
    "#%matplotlib inline\n",
    "#from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "#import seaborn as sns\n",
    "#sns.set_context('poster')\n",
    "#colors = sns.color_palette(\"cubehelix\", 5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import scipy.stats as stats\n",
    "#from scipy.stats import norm\n",
    "#import sklearn\n",
    "\n",
    "#from importlib import reload\n",
    "\n",
    "##################################\n",
    "\n",
    "#import itertools\n",
    "import nibabel as nib\n",
    "#import brainiak\n",
    "#import json\n",
    "#import _pickle as cPickle\n",
    "#import analysis_helpers as helpers\n",
    "#from nilearn import image\n",
    "#from numpy import shape\n",
    "from brainiak.searchlight.searchlight import Searchlight\n",
    "from brainiak.searchlight.searchlight import Diamond\n",
    "#from sklearn.metrics.pairwise import pairwise_distances as pd\n",
    "#from timeit import default_timer as timer\n",
    "#from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global parameters\n",
    "sub_list = ['0110171', '0110172', '0111171', '0112171', '0112172', '0112173',\n",
    "       '0113171', '0115174', '0117171', '0118171', '0118172', '0119171',\n",
    "       '0119172', '0119173', '0119174', '0120171', '0120172', '0120173',\n",
    "       '0123171', '0123173', '0124171', '0125171', '0125172', '1121161',\n",
    "       '1130161', '1202161', '1203161', '1206161', '1206162', '1206163',\n",
    "       '1207162']\n",
    "iv = 'time_point'\n",
    "normalize_on = True\n",
    "logged = True\n",
    "SAVE_PATH = '/jukebox/ntb/projects/sketchloop02/data/searchlight_output/'\n",
    "RM, RF, DM, DF, trained_objs, control_objs, sub = None, None, None, None, None, None, None\n",
    "\n",
    "affine = np.array([[-1.996683955192566, -0.026332620531320572, -0.11206881701946259, 91.78023529052734],\n",
    "[-0.026291240006685257, 1.9998265504837036, -0.0014756681630387902, -125.46440124511719],\n",
    "[-0.11207851767539978, 7.630718279472148e-09, 1.9968571662902832, -120.91204833984375],\n",
    "[0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "def load_recog_data(subject):\n",
    "    features = np.load('/jukebox/ntb/projects/sketchloop02/data/feature_matrices_and_metadata/' +\n",
    "                       subject + '_12_featurematrix.npy')\n",
    "    return features\n",
    "\n",
    "def load_recog_metadata(subject):\n",
    "    metadata = pd.read_csv(\n",
    "        '/jukebox/ntb/projects/sketchloop02/data/feature_matrices_and_metadata/metadata_' +\n",
    "        subject + '_V1_12.csv')\n",
    "    return metadata\n",
    "\n",
    "def load_draw_metadata(subject):\n",
    "    metadata = pd.read_csv(\n",
    "        '/jukebox/ntb/projects/sketchloop02/data/feature_matrices_and_metadata/metadata_' +\n",
    "        subject + '_drawing.csv')\n",
    "    return metadata\n",
    "\n",
    "def load_draw_data(subject):\n",
    "    features = np.load('/jukebox/ntb/projects/sketchloop02/data/feature_matrices_and_metadata/' +\n",
    "                       subject + '_featurematrix.npy')\n",
    "    return features\n",
    "\n",
    "def makemask(coordinates):\n",
    "    dims = np.shape(coordinates)\n",
    "    mask = np.ma.make_mask_none((88, 128, 128))\n",
    "    for x, y, z in itertools.product(range(dims[0]), range(dims[1]), range(dims[2])):\n",
    "        a, b, c = coordinates[x, y, z]\n",
    "        mask[a, b, c] = True\n",
    "    return mask\n",
    "\n",
    "def maskfeatures(mask, features):\n",
    "    x = [0] * features.shape[0]\n",
    "    for i, n in enumerate(f[mask] for f in features):\n",
    "        x[i] = n\n",
    "    x = np.array(x)\n",
    "    return x\n",
    "    return np.array([f[mask] for f in features])\n",
    "\n",
    "# z-score normalization to de-mean & standardize variances within-voxel\n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "def nan_if(arr, value):\n",
    "    return np.where(arr == value, np.nan, arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis / Data Collection\n",
    "How searchlight works with brainiak:\n",
    "\n",
    "1. Initiate a searchlight object, articulating certain parameters (e.g., searchlight shape, radius, max edge length, in voxels, of the 3D block(?)).\n",
    "2. Distribute data to be searched to the searchlight object, sorting between MPI ranks (idk what MPI ranks are tbh).\n",
    "3. Broadcast data, i.e., define other variables to be available for each execution of the searchlight function.\n",
    "4. Run the searchlight, this time articulating as a parameter the function to be applied at each searchlight location.\n",
    "\n",
    "We want to perform searchlight again, but this time the objective is to perform a version of helpers.make_drawing_predictions on each searchlight mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_drawing_predictions(subject_data, mask, sl_rad, bcast_var):\n",
    "    # format the train/test split\n",
    "    t1, t2 = trained_objs\n",
    "    subject_data = subject_data[0].transpose((3, 0, 1, 2))\n",
    "    X_train = subject_data[920:,:].reshape((160, -1))\n",
    "    X_test = subject_data[:920,:].reshape((920, -1))\n",
    "    y_train = RM.label.values\n",
    "    y_test = DM.label.values\n",
    "    \n",
    "    # normalize if we want\n",
    "    if normalize_on:\n",
    "        X_train = normalize(X_train)\n",
    "        X_test = normalize(X_test)\n",
    "\n",
    "    # single train/test split\n",
    "    clf = linear_model.LogisticRegression(penalty='l2',C=1).fit(X_train, y_train)\n",
    "    \n",
    "    ## add prediction probabilities to metadata matrix\n",
    "    ## must sort so that trained are first, and control is last\n",
    "    ## also save out new columns in the same order\n",
    "    cats = list(clf.classes_)\n",
    "    _ordering = np.argsort(np.hstack((trained_objs,control_objs))) ## e.g., [chair table bench bed] ==> [3 2 0 1]\n",
    "    ordering = np.argsort(_ordering) ## get indices that sort from alphabetical to (trained_objs, control_objs)\n",
    "    probs = (np.log(clf.predict_proba(X_test)) if logged else clf.predict_proba(X_test))\n",
    "    \n",
    "    out = probs[:,ordering]\n",
    "    DM['t1_prob'] = out[:,0]\n",
    "    DM['t2_prob'] = out[:,1]\n",
    "    DM['c1_prob'] = out[:,2]\n",
    "    DM['c2_prob'] = out[:,3]\n",
    "    DM['bed_prob'] = probs[:,0]\n",
    "    DM['bench_prob'] = probs[:,1]\n",
    "    DM['chair_prob'] = probs[:,2]\n",
    "    DM['table_prob'] = probs[:,3]\n",
    "    \n",
    "    target = np.vstack((DM[DM.label==t1].groupby(iv)['t1_prob'].mean().values,\n",
    "                       DM[DM.label==t2].groupby(iv)['t2_prob'].mean().values)).mean(0)\n",
    "    foil = np.vstack((DM[DM.label==t1].groupby(iv)['t2_prob'].mean().values,\n",
    "                       DM[DM.label==t2].groupby(iv)['t1_prob'].mean().values)).mean(0)\n",
    "    \n",
    "    return np.mean(target-foil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that sets up and organizes searchlight over a set of subjects\n",
    "def searchlight_over_each(subjects, sl_rad):\n",
    "    global RM, RF, DM, DF, trained_objs, control_objs, sub\n",
    "    \n",
    "    for s in subjects:\n",
    "        sub = s\n",
    "        print(s)\n",
    "        \n",
    "        # set up an instance of the searchlight class\n",
    "        sl = Searchlight(sl_rad=sl_rad, shape=Diamond)\n",
    "        \n",
    "        # arrange data to be distributed to searchlight\n",
    "        ### load subject data in\n",
    "        RM, DM = load_recog_metadata(s),  load_draw_metadata(s)\n",
    "        CF = np.concatenate((load_draw_data(s), load_recog_data(s).transpose((1, 0, 2, 3))), axis=0).transpose((1, 2, 3, 0))\n",
    "        \n",
    "        ### identify control objects;\n",
    "        ### we wil train one classifier with\n",
    "        trained_objs = np.unique(DM.label.values)\n",
    "        control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        \n",
    "        # distribute and broadcast needed data to searchlight\n",
    "        print('distributing data...')\n",
    "        #start = timer()\n",
    "        sl.distribute([CF], np.ones((88, 128, 128)))\n",
    "        sl.broadcast(None)\n",
    "        #end = timer()\n",
    "        #print('time to distribute:', end - start)\n",
    "        \n",
    "        # run searchlight\n",
    "        print('running...')\n",
    "        #start = timer()\n",
    "        subject_outputs = np.array(sl.run_searchlight(make_drawing_predictions))\n",
    "        #end = timer()\n",
    "        print('time to run searchlight:', end - start)\n",
    "        \n",
    "        # store output for this subject\n",
    "        nib.save(nib.Nifti1Image(np.array(sl.run_searchlight(make_drawing_predictions)).astype(np.float32), affine),\n",
    "                 SAVE_PATH + s + 'clf_searchlight.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0110171\n",
      "distributing data...\n",
      "running...\n"
     ]
    }
   ],
   "source": [
    "# run searchlight over all subjects and print time to compute results\n",
    "searchlight_over_each(sub_list,  5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of Maps into Standard Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Jupyter Magic for executing terminal commands, call FSL's flirt function on each searchlight map to convert them into standard space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nilearn import plotting\n",
    "\n",
    "for sub in sub_list:\n",
    "    print(sub)\n",
    "    flirtin = os.path.join(SAVE_PATH, sub+'clf_searchlight.nii.gz')\n",
    "    flirtout = os.path.join(SAVE_PATH, sub+'_standard_clf_searchlight.nii.gz')\n",
    "    flirtref = '/jukebox/ntb/projects/sketchloop02/searchlight_outputs/MNI152_T1_2mm_brain.nii.gz'\n",
    "    flirtinit = '/jukebox/ntb/users/jwammes/sketchloop/subjects/' + sub + '_neurosketch/analysis/firstlevel/draw_run_1.feat/reg/highres2standard.mat'\n",
    "    #flirtinit = '/jukebox/ntb/projects/sketchloop02/subjects/' + sub + '_neurosketch/analysis/firstlevel/reg_recognition_run_1.feat/reg/highres2standard.mat'\n",
    "    #flirtinit = '/jukebox/ntb/projects/sketchloop02/subjects/' + sub + '_neurosketch/analysis/firstlevel/preproc_recognition_run_5.feat/reg/highres2standard.mat'\n",
    "    !module load fsl; flirt -in \"$flirtin\" -out \"$flirtout\" -ref \"$flirtref\" -applyxfm -init \"$flirtinit\"\n",
    "    \n",
    "    localizer_tmap_filename = os.path.join(SAVE_PATH, sub+'_standard_clf_searchlight.nii.gz')\n",
    "    plotting.plot_glass_brain(localizer_tmap_filename, #bg_img=None,#bg_img='/jukebox/ntb/projects/sketchloop02/searchlight_outputs/MNI152_T1_2mm_brain.nii.gz',\n",
    "                            threshold='auto')\n",
    "    plotting.plot_stat_map(localizer_tmap_filename, #bg_img=None, #bg_img='/jukebox/ntb/projects/sketchloop02/searchlight_outputs/MNI152_T1_2mm_brain.nii.gz',\n",
    "                           threshold='auto', title=\"plot_stat_map\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
