{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import linear_model, datasets, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define path to input datasets (tidy format)\n",
    "path_to_recog = '/home/jefan/neurosketch/neurosketch_voxelmat_freesurfer_recog'\n",
    "# path_to_recog = '/home/jgunn/neurosketch/recmatrices'\n",
    "path_to_draw = '/home/jefan/neurosketch/neurosketch_voxelmat_freesurfer_drawing' \n",
    "# path_to_draw = '/home/jgunn/neurosketch/drawmatrices' \n",
    "roi_list = np.array([\"V1\", \"V2\", \"LOC\", \"IT\", \"fusiform\", \"parahippo\",  \"PRC\",  \"ento\", \"hipp\", 'mOFC', 'IFG', 'rostMFG', 'caudMFG', 'precentral', 'SMG', 'STG'])\n",
    "# roi_formatted = np.array([\"V1\", \"V2\", \"LOC\", \"IT\", \"fusiform\", \"para\\nhippo\",  \"PRC\",  \"ento\", \"hipp\", 'mOFC', 'IFG', 'rost\\nMFG', 'caud\\nMFG', 'pre\\ncentral', 'SMG', 'STG'])\n",
    "\n",
    "## get raw file list for recognition runs\n",
    "RECOG_METAS = sorted([i for i in os.listdir(path_to_recog) if i.split('.')[-1]=='csv'])\n",
    "RECOG_FEATS = sorted([i for i in os.listdir(path_to_recog) if i.split('.')[-1]=='npy'])\n",
    "RECOG_SUBS = np.array([i.split('_')[0] for i in RECOG_FEATS])\n",
    "\n",
    "recog_sub_list = np.unique(RECOG_SUBS)\n",
    "\n",
    "def preprocess_recog(RECOG_METAS, RECOG_FEATS):\n",
    "    M = [i for i in RECOG_METAS if len(i.split('.')[0].split('_'))==4]\n",
    "    F = [i for i in RECOG_FEATS if len(i.split('.')[0].split('_'))==4]\n",
    "    return M,F\n",
    "\n",
    "RECOG_METAS, RECOG_FEATS = preprocess_recog(RECOG_METAS, RECOG_FEATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get raw file list for drawing runs\n",
    "DRAW_METAS = sorted([i for i in os.listdir(path_to_draw) if i.split('.')[-1]=='csv'])\n",
    "DRAW_FEATS = sorted([i for i in os.listdir(path_to_draw) if i.split('.')[-1]=='npy'])\n",
    "DRAW_SUBS = np.array([i.split('_')[0] for i in DRAW_FEATS])\n",
    "draw_sub_list = np.unique(DRAW_SUBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subs: 30\n"
     ]
    }
   ],
   "source": [
    "## get subject ID's that have complete datasets from all phases of experiment\n",
    "sub_list = np.intersect1d(recog_sub_list,draw_sub_list)\n",
    "sub_list = [s for s in sub_list if s != '1207162']\n",
    "print('Number of subs: {}'.format(len(sub_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## filter file list so only contains the sessions that have full datasets\n",
    "def extract_good_sessions(DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS):\n",
    "    _DRAW_METAS = [i for i in DRAW_METAS if i.split('_')[1] in sub_list]\n",
    "    _DRAW_FEATS = [i for i in DRAW_FEATS if i.split('_')[0] in sub_list]\n",
    "    _RECOG_METAS = [i for i in RECOG_METAS if i.split('_')[1] in sub_list]\n",
    "    _RECOG_FEATS = [i for i in RECOG_FEATS if i.split('_')[0] in sub_list]\n",
    "    return _DRAW_METAS, _DRAW_FEATS, _RECOG_METAS, _RECOG_FEATS\n",
    "\n",
    "DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS =  \\\n",
    "extract_good_sessions(DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS)\n",
    "\n",
    "RECOG_SUBS = np.array([i.split('_')[0]+'_neurosketch' for i in RECOG_FEATS])\n",
    "RECOG_ROIS = np.array([i.split('_')[1] for i in RECOG_FEATS])\n",
    "\n",
    "DRAW_SUBS = np.array([i.split('_')[0]+'_neurosketch' for i in DRAW_FEATS])\n",
    "DRAW_ROIS = np.array([i.split('_')[1] for i in DRAW_FEATS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do we do at classifying the target when we train on recognition patterns only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Helper data loader functions\n",
    "def load_draw_meta(this_sub):\n",
    "    this_file = 'metadata_{}_drawing.csv'.format(this_sub)\n",
    "    x = pd.read_csv(os.path.join(path_to_draw,this_file))\n",
    "    x = x.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "    x['trial_num'] = np.repeat(np.arange(40),23)        \n",
    "    return x\n",
    "    \n",
    "def load_draw_feats(this_sub,this_roi):\n",
    "    this_file = '{}_{}_featurematrix.npy'.format(this_sub,this_roi)\n",
    "    y = np.load(os.path.join(path_to_draw,this_file))\n",
    "    y = y.transpose()\n",
    "    return y\n",
    "\n",
    "def load_draw_data(this_sub,this_roi):\n",
    "    x = load_draw_meta(this_sub)\n",
    "    y = load_draw_feats(this_sub,this_roi)\n",
    "    assert y.shape[0] == x.shape[0]    \n",
    "    return x,y\n",
    "\n",
    "def load_recog_meta(this_sub,this_roi,this_phase):\n",
    "    this_file = 'metadata_{}_{}_{}.csv'.format(this_sub,this_roi,this_phase)\n",
    "    x = pd.read_csv(os.path.join(path_to_recog,this_file))\n",
    "    x = x.drop(['Unnamed: 0'], axis=1)\n",
    "    return x\n",
    "    \n",
    "def load_recog_feats(this_sub,this_roi,this_phase):\n",
    "    this_file = '{}_{}_{}_featurematrix.npy'.format(this_sub,this_roi,this_phase)\n",
    "    y = np.load(os.path.join(path_to_recog,this_file))\n",
    "    y = y.transpose()\n",
    "    return y    \n",
    "\n",
    "def load_recog_data(this_sub,this_roi,this_phase):\n",
    "    x = load_recog_meta(this_sub,this_roi,this_phase)\n",
    "    y = load_recog_feats(this_sub,this_roi,this_phase)\n",
    "    assert y.shape[0] == x.shape[0]    \n",
    "    return x,y\n",
    "\n",
    "# z-score normalization to de-mean & standardize variances within-voxel \n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "## plotting helper\n",
    "def get_prob_timecourse(iv,DM):\n",
    "    trained_objs = np.unique(DM.label.values)\n",
    "    t1 = trained_objs[0]\n",
    "    t2 = trained_objs[1]\n",
    "    target = np.vstack((DM[DM.label==t1].groupby(iv)['t1_prob'].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['t2_prob'].mean().values)).mean(0) ## target timecourse; mean is taken over what?\n",
    "    foil = np.vstack((DM[DM.label==t1].groupby(iv)['t2_prob'].mean().values,\n",
    "                    DM[DM.label==t2].groupby(iv)['t1_prob'].mean().values)).mean(0) ## foil timecourse\n",
    "    control = np.vstack((DM[DM.label==t1].groupby(iv)['ctrl_prob'].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['ctrl_prob'].mean().values)).mean(0) ## control timecourse\n",
    "    \n",
    "    return target, foil, control\n",
    "     \n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inds =(ALLDM['roi']=='V1') & (ALLDM['subj']=='0110171') \n",
    "# get_prob_timecourse('time_point',ALLDM[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## general plotting params\n",
    "sns.set_context('poster')\n",
    "colors = sns.color_palette(\"cubehelix\", 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## example subject, roi, and phase (localizer is default)\n",
    "this_sub = sub_list[0]\n",
    "this_roi = roi_list[2] ## order is: ['V1','V2','LOC','IT','fusiform','parahippo', 'PRC', 'ento','hipp','mOFC']\n",
    "this_phase = '12' ## options are '12', '34', '56'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "V2\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n"
     ]
    }
   ],
   "source": [
    "ALLDM = []\n",
    "\n",
    "## loop through all subjects and rois\n",
    "Acc = []\n",
    "for this_roi in roi_list:\n",
    "    print (this_roi)\n",
    "    acc = []\n",
    "    for this_sub in sub_list:\n",
    "        print(this_sub)\n",
    "        ## load subject data in\n",
    "        RM12, RF12 = load_recog_data(this_sub,this_roi,'12')\n",
    "        RM34, RF34 = load_recog_data(this_sub,this_roi,'34')        \n",
    "        RM = pd.concat([RM12,RM34])\n",
    "        RF = np.vstack((RF12,RF34))        \n",
    "        DM, DF = load_draw_data(this_sub,this_roi)\n",
    "        assert RF.shape[1]==DF.shape[1] ## that number of voxels is identical\n",
    "        \n",
    "        # identify control objects; \n",
    "        # we wil train one classifier with \n",
    "        trained_objs = np.unique(DM.label.values)\n",
    "        control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        probs = []\n",
    "        \n",
    "        for ctrl in control_objs:\n",
    "            \n",
    "            inds = RM.label != ctrl\n",
    "            _RM = RM[inds]\n",
    "\n",
    "            ## normalize voxels within task\n",
    "            normalize_on = 1\n",
    "            if normalize_on:\n",
    "                _RF = normalize(RF[inds,:])\n",
    "                _DF = normalize(DF)\n",
    "            else:\n",
    "                _RF = RF[inds,:]\n",
    "                _DF = DF\n",
    "\n",
    "            # single train/test split\n",
    "            X_train = _RF # recognition run feature set\n",
    "            y_train = _RM.label.values # list of labels for the training set\n",
    "\n",
    "            ## subset timepoints?\n",
    "            inds = DM.time_point>0 # all timepoints are > 0, so no subsetting happens\n",
    "            _DF = _DF[inds,:]\n",
    "            DM = DM[inds]\n",
    "\n",
    "            X_test = _DF\n",
    "            y_test = DM.label.values\n",
    "            # clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "            clf = linear_model.LogisticRegression(penalty='l2',C=1).fit(X_train, y_train)    \n",
    "\n",
    "            ## add prediction probabilities to metadata matrix\n",
    "            ## must sort so that trained are first, and control is last\n",
    "            cats = list(clf.classes_)\n",
    "            ctrl_index = cats.index([c for c in control_objs if c != ctrl][0])\n",
    "            t1_index = cats.index(trained_objs[0])\n",
    "            t2_index = cats.index(trained_objs[1])\n",
    "            ordering = np.argsort([t1_index, t2_index, ctrl_index])\n",
    "            probs.append(clf.predict_proba(X_test)[:,ordering])\n",
    "\n",
    "        DM['t1_prob'] = (probs[0][:,0] + probs[1][:,0])/2.0\n",
    "        DM['t2_prob'] = (probs[0][:,1] + probs[1][:,1])/2.0\n",
    "        DM['ctrl_prob'] = (probs[0][:,2] + probs[1][:,2])/2.0\n",
    "        \n",
    "        DM['subj'] = np.repeat(this_sub,DM.shape[0])\n",
    "        DM['roi'] = np.repeat(this_roi,DM.shape[0])\n",
    "        \n",
    "        if len(ALLDM)==0:\n",
    "            ALLDM = DM\n",
    "        else:\n",
    "            ALLDM = pd.concat([ALLDM,DM],ignore_index=True)\n",
    "\n",
    "        ## plot probability timecourse\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        iv = 'run_num'\n",
    "        t,f,c = get_prob_timecourse(iv,DM)\n",
    "        plt.plot(t,color=colors[0],label='target')\n",
    "        plt.plot(f,color=colors[1],label='foil')\n",
    "        plt.plot(c,color=colors[2],label='control')\n",
    "        plt.legend(bbox_to_anchor=(1.45, 1.01))\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel(iv)\n",
    "        plt.ylabel('probability')\n",
    "        if not os.path.exists('./plots/subj'):\n",
    "            os.makedirs('./plots/subj')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./plots/subj/{}_{}_prob_{}.pdf'.format(iv.split('_')[0],this_roi,this_sub))\n",
    "        plt.close(fig)\n",
    "        acc.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    Acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        #trained_objs = np.unique(DM.label.values)\n",
    "        #control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        #inds = DM.label != 'chair' # all timepoints are > 0, so no subsetting happens\n",
    "        \n",
    "#np.shape(probs)\n",
    "#(probs[0][:,0] + probs[1][:,0])/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['Unnamed: 0.1.1'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-27d9a97c0108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mALLDM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALLDM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0.1.1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2048\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3574\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3575\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3576\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3577\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['Unnamed: 0.1.1'] not contained in axis"
     ]
    }
   ],
   "source": [
    "ALLDM = ALLDM.drop(['Unnamed: 0.1.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>LOC</th>\n",
       "      <th>IT</th>\n",
       "      <th>fusiform</th>\n",
       "      <th>para\n",
       "hippo</th>\n",
       "      <th>PRC</th>\n",
       "      <th>ento</th>\n",
       "      <th>hipp</th>\n",
       "      <th>mOFC</th>\n",
       "      <th>IFG</th>\n",
       "      <th>rost\n",
       "MFG</th>\n",
       "      <th>caud\n",
       "MFG</th>\n",
       "      <th>pre\n",
       "central</th>\n",
       "      <th>SMG</th>\n",
       "      <th>STG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.270652</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.305435</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.343478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305435</td>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.307609</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.339130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.307609</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.360870</td>\n",
       "      <td>0.332609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.272826</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.332609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.296739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.476087</td>\n",
       "      <td>0.484783</td>\n",
       "      <td>0.444565</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.296739</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.383696</td>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.303261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.406522</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.360870</td>\n",
       "      <td>0.295652</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.342391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.526087</td>\n",
       "      <td>0.454348</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.372826</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.409783</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.382609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.394565</td>\n",
       "      <td>0.398913</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.370652</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.389130</td>\n",
       "      <td>0.330435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.397826</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.303261</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.328261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.527174</td>\n",
       "      <td>0.533696</td>\n",
       "      <td>0.420652</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.384783</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.284783</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.270652</td>\n",
       "      <td>0.365217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.372826</td>\n",
       "      <td>0.421739</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.385870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.415217</td>\n",
       "      <td>0.459783</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.323913</td>\n",
       "      <td>0.379348</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.289130</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.344565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.285870</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.381522</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.285870</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.371739</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.340217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.388043</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.329348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.294565</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.317391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.511957</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.267391</td>\n",
       "      <td>0.410870</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.305435</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.303261</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.310870</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>0.257609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.309783</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.384783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.460870</td>\n",
       "      <td>0.491304</td>\n",
       "      <td>0.383696</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.363043</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.333696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.444565</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.416304</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.323913</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.366304</td>\n",
       "      <td>0.294565</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.295652</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.329348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.463043</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.296739</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.363043</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.346739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.459783</td>\n",
       "      <td>0.451087</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.301087</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.301087</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.318478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.292391</td>\n",
       "      <td>0.273913</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.280435</td>\n",
       "      <td>0.360870</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.377174</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.327174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.292391</td>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.309783</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.376087</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.323913</td>\n",
       "      <td>0.290217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.406522</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.317391</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.392391</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.313043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.515217</td>\n",
       "      <td>0.409783</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.303261</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>0.301087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.289130</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.371739</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.371739</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.366304</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.281522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.396739</td>\n",
       "      <td>0.431522</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.290217</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.376087</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.335870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.359783</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.376087</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.361957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2       LOC        IT  fusiform  para\\nhippo       PRC  \\\n",
       "0   0.333696  0.369565  0.368478  0.343478  0.304348     0.342391  0.357609   \n",
       "1   0.305435  0.316304  0.364130  0.339130  0.358696     0.334783  0.330435   \n",
       "2   0.332609  0.329348  0.297826  0.382609  0.380435     0.320652  0.354348   \n",
       "3   0.408696  0.391304  0.272826  0.338043  0.354348     0.343478  0.326087   \n",
       "4   0.319565  0.382609  0.338043  0.336957  0.326087     0.342391  0.348913   \n",
       "5   0.476087  0.484783  0.444565  0.338043  0.336957     0.339130  0.319565   \n",
       "6   0.391304  0.378261  0.406522  0.342391  0.331522     0.302174  0.278261   \n",
       "7   0.526087  0.454348  0.426087  0.338043  0.341304     0.321739  0.332609   \n",
       "8   0.394565  0.398913  0.369565  0.370652  0.339130     0.346739  0.342391   \n",
       "9   0.397826  0.356522  0.319565  0.311957  0.303261     0.332609  0.334783   \n",
       "10  0.527174  0.533696  0.420652  0.365217  0.384783     0.341304  0.345652   \n",
       "11  0.369565  0.372826  0.421739  0.330435  0.354348     0.300000  0.352174   \n",
       "12  0.415217  0.459783  0.333696  0.338043  0.348913     0.311957  0.320652   \n",
       "13  0.311957  0.285870  0.334783  0.342391  0.381522     0.367391  0.346739   \n",
       "14  0.388043  0.382609  0.341304  0.320652  0.328261     0.339130  0.320652   \n",
       "15  0.338043  0.393478  0.336957  0.329348  0.368478     0.308696  0.331522   \n",
       "16  0.443478  0.511957  0.417391  0.267391  0.410870     0.320652  0.305435   \n",
       "17  0.316304  0.339130  0.352174  0.304348  0.353261     0.330435  0.353261   \n",
       "18  0.334783  0.358696  0.382609  0.351087  0.402174     0.328261  0.329348   \n",
       "19  0.460870  0.491304  0.383696  0.330435  0.335870     0.328261  0.354348   \n",
       "20  0.444565  0.450000  0.416304  0.345652  0.352174     0.331522  0.342391   \n",
       "21  0.475000  0.463043  0.331522  0.288043  0.354348     0.296739  0.339130   \n",
       "22  0.459783  0.451087  0.357609  0.329348  0.301087     0.339130  0.342391   \n",
       "23  0.304348  0.325000  0.328261  0.292391  0.273913     0.327174  0.313043   \n",
       "24  0.346739  0.365217  0.291304  0.286957  0.322826     0.292391  0.316304   \n",
       "25  0.378261  0.368478  0.406522  0.306522  0.352174     0.308696  0.329348   \n",
       "26  0.408696  0.515217  0.409783  0.345652  0.347826     0.343478  0.302174   \n",
       "27  0.289130  0.311957  0.330435  0.269565  0.371739     0.340217  0.330435   \n",
       "28  0.396739  0.431522  0.368478  0.327174  0.356522     0.335870  0.368478   \n",
       "29  0.335870  0.354348  0.333696  0.368478  0.375000     0.359783  0.369565   \n",
       "\n",
       "        ento      hipp      mOFC       IFG  rost\\nMFG  caud\\nMFG  \\\n",
       "0   0.348913  0.335870  0.343478  0.315217   0.270652   0.357609   \n",
       "1   0.346739  0.331522  0.340217  0.331522   0.348913   0.342391   \n",
       "2   0.333696  0.307609  0.351087  0.380435   0.319565   0.325000   \n",
       "3   0.328261  0.313043  0.314130  0.348913   0.367391   0.340217   \n",
       "4   0.354348  0.311957  0.336957  0.320652   0.318478   0.357609   \n",
       "5   0.331522  0.326087  0.314130  0.296739   0.338043   0.343478   \n",
       "6   0.343478  0.336957  0.360870  0.295652   0.332609   0.342391   \n",
       "7   0.326087  0.372826  0.338043  0.351087   0.314130   0.409783   \n",
       "8   0.341304  0.356522  0.336957  0.335870   0.346739   0.317391   \n",
       "9   0.361957  0.313043  0.346739  0.354348   0.350000   0.335870   \n",
       "10  0.342391  0.375000  0.325000  0.284783   0.343478   0.335870   \n",
       "11  0.356522  0.336957  0.318478  0.334783   0.320652   0.329348   \n",
       "12  0.297826  0.332609  0.340217  0.323913   0.379348   0.339130   \n",
       "13  0.285870  0.322826  0.353261  0.356522   0.342391   0.368478   \n",
       "14  0.321739  0.341304  0.300000  0.313043   0.330435   0.351087   \n",
       "15  0.294565  0.345652  0.330435  0.308696   0.306522   0.332609   \n",
       "16  0.318478  0.344565  0.303261  0.348913   0.325000   0.310870   \n",
       "17  0.311957  0.336957  0.346739  0.309783   0.340217   0.342391   \n",
       "18  0.326087  0.319565  0.358696  0.332609   0.331522   0.315217   \n",
       "19  0.333696  0.326087  0.286957  0.356522   0.363043   0.334783   \n",
       "20  0.323913  0.318478  0.340217  0.366304   0.294565   0.334783   \n",
       "21  0.322826  0.363043  0.315217  0.302174   0.338043   0.291304   \n",
       "22  0.301087  0.352174  0.317391  0.355435   0.331522   0.330435   \n",
       "23  0.361957  0.280435  0.360870  0.321739   0.319565   0.344565   \n",
       "24  0.309783  0.367391  0.376087  0.361957   0.314130   0.351087   \n",
       "25  0.356522  0.315217  0.308696  0.338043   0.317391   0.330435   \n",
       "26  0.329348  0.351087  0.303261  0.320652   0.341304   0.361957   \n",
       "27  0.332609  0.304348  0.371739  0.313043   0.352174   0.316304   \n",
       "28  0.343478  0.332609  0.290217  0.361957   0.365217   0.336957   \n",
       "29  0.344565  0.321739  0.302174  0.376087   0.355435   0.339130   \n",
       "\n",
       "    pre\\ncentral       SMG       STG  \n",
       "0       0.305435  0.321739  0.343478  \n",
       "1       0.307609  0.344565  0.339130  \n",
       "2       0.343478  0.360870  0.332609  \n",
       "3       0.336957  0.330435  0.332609  \n",
       "4       0.341304  0.339130  0.296739  \n",
       "5       0.383696  0.378261  0.303261  \n",
       "6       0.358696  0.369565  0.342391  \n",
       "7       0.365217  0.282609  0.382609  \n",
       "8       0.319565  0.389130  0.330435  \n",
       "9       0.348913  0.344565  0.328261  \n",
       "10      0.351087  0.270652  0.365217  \n",
       "11      0.313043  0.345652  0.385870  \n",
       "12      0.289130  0.356522  0.344565  \n",
       "13      0.371739  0.333696  0.340217  \n",
       "14      0.327174  0.346739  0.329348  \n",
       "15      0.339130  0.302174  0.317391  \n",
       "16      0.291304  0.293478  0.257609  \n",
       "17      0.328261  0.326087  0.350000  \n",
       "18      0.336957  0.326087  0.384783  \n",
       "19      0.332609  0.317391  0.333696  \n",
       "20      0.295652  0.308696  0.329348  \n",
       "21      0.339130  0.353261  0.346739  \n",
       "22      0.282609  0.353261  0.318478  \n",
       "23      0.377174  0.304348  0.327174  \n",
       "24      0.352174  0.323913  0.290217  \n",
       "25      0.392391  0.339130  0.313043  \n",
       "26      0.332609  0.373913  0.301087  \n",
       "27      0.366304  0.327174  0.281522  \n",
       "28      0.376087  0.342391  0.335870  \n",
       "29      0.314130  0.361957  0.361957  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Acc = np.array(Acc) # 10 ROIs, 31 subjects. A score representing classification performance for that subject/roi pair\n",
    "x = pd.DataFrame(Acc.transpose())\n",
    "x.columns = roi_formatted\n",
    "\n",
    "x # x is acc formatted into a ROI x subject dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAEcCAYAAAD3Hm1oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c0d8336d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(11,4))\n",
    "sns.barplot(data=x,palette='husl',ci=95) # very amazing thing sns must be!\n",
    "plt.axhline(1/3,linestyle=':',color='k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test on drawing runs; train on recognition runs')\n",
    "plt.ylim(0,0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALLDM.to_csv('./logistic_timeseries_neural_vgg.csv') ## train recog, test drawing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subj</th>\n",
       "      <th>label</th>\n",
       "      <th>run_num</th>\n",
       "      <th>TR_num</th>\n",
       "      <th>time_point</th>\n",
       "      <th>trial_num</th>\n",
       "      <th>t1_prob</th>\n",
       "      <th>t2_prob</th>\n",
       "      <th>ctrl_prob</th>\n",
       "      <th>roi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0110171</td>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.849860</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0110171</td>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969912</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0110171</td>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371163</td>\n",
       "      <td>0.179743</td>\n",
       "      <td>0.449093</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0110171</td>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248547</td>\n",
       "      <td>0.407146</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0110171</td>\n",
       "      <td>bed</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.540043</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.455972</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subj label  run_num  TR_num  time_point  trial_num   t1_prob   t2_prob  \\\n",
       "0  0110171   bed        1      10           1          0  0.849860  0.011288   \n",
       "1  0110171   bed        1      11           2          0  0.969912  0.008953   \n",
       "2  0110171   bed        1      12           3          0  0.371163  0.179743   \n",
       "3  0110171   bed        1      13           4          0  0.248547  0.407146   \n",
       "4  0110171   bed        1      14           5          0  0.540043  0.003985   \n",
       "\n",
       "   ctrl_prob roi  \n",
       "0   0.138852  V1  \n",
       "1   0.021136  V1  \n",
       "2   0.449093  V1  \n",
       "3   0.344307  V1  \n",
       "4   0.455972  V1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALLDM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how does this code work?\n",
    "\n",
    "`lookup` is weird.\n",
    "\n",
    "Scans through `['run_num','trial_num','time_point']`, then ROIs, then subjects too.\n",
    "\n",
    "Now what is T F C? Target Foil Control. uses the helper function on a subset of ALLDMS that focuses on this roi and this subject. How does that function work?\n",
    "seems to first take the unique set of labels..? How is he telling between target and off-target objs? Seems there's an ordering, and unique preserves it? Drawing trials will always only be the trained ones. Duh. Control is never the accurate thing, but it always gets assigned a score. This'll make the other coding problem easier.\n",
    "\n",
    "So it isolates by roi, subject and object, groups iteratively by ['run_num','trial_num','time_point'] and takes the mean across those groupings for each object category. So I get 4 means for 4 runs, and 3 sets of means, one for each condition. Need more focus on how foil is calculated, though. It's just t2, that's all.\n",
    "\n",
    "So let's start over knowing what I know. Scanning through the units I want my timecourse taken over (runs, trials, timepoints), then over ROIs, then over subjects. On the subject level, I'm getting the probability time course over the current iv for trained, foil, and control. For each sub, store said timecourse in T, F, or C. DTF DTC and DFC are the differences from D to F, etc. If render cond is positive, the plot will be trained, foil, control. Otherwise, it's DTF and so forth. \n",
    "\n",
    "Then generate graph for this. DOn't know if I need to know the details. I'm looking to set up the final graphs. Instead of drawing from mean over this probability time course, draw the max or the max difference. \n",
    "\n",
    "Seems I also need to do this for DTF over repetitions. Take correlations of these for each subject (rather than max or mean). ANd then plot those correlations. Where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "this_roi = 'V1'\n",
    "subs = np.unique(ALLDM.subj.values) # list of subjects pulled from ALLDM.\n",
    "lookup = dict(zip(['trial_num','run_num','time_point'],['repetition','run','TR']))\n",
    "ivs = ['run_num','trial_num','time_point']\n",
    "this_iv = 'time_point'\n",
    "\n",
    "## do you want to render the CONDITION-wise plots -- trained vs. foil vs control\n",
    "## or the DIFFERENCE plots -- trained - foil vs foil - control?\n",
    "render_cond = 1\n",
    "\n",
    "for this_iv in ivs:\n",
    "    for this_roi in roi_list:\n",
    "\n",
    "        T = []\n",
    "        F = []\n",
    "        C = []\n",
    "        Sub = []\n",
    "        for sub in subs:\n",
    "            inds =(ALLDM['roi']==this_roi) & (ALLDM['subj']==sub) \n",
    "            t,f,c = get_prob_timecourse(this_iv,ALLDM[inds])\n",
    "            if len(T)==0:\n",
    "                T = t\n",
    "                F = f\n",
    "                C = c\n",
    "                DTF = t-f                \n",
    "                DTC = t-c\n",
    "                DFC = f-c\n",
    "            else:\n",
    "                T = np.hstack((T,t))\n",
    "                F = np.hstack((F,f))        \n",
    "                C = np.hstack((C,c)) \n",
    "                DTF = np.hstack((DTF,t-f))                \n",
    "                DTC = np.hstack((DTC,t-c))\n",
    "                DFC = np.hstack((DFC,f-c))\n",
    "            Sub.append([sub]*len(t))   \n",
    "        \n",
    "        if render_cond==1:\n",
    "            ## make longform version of dataframe to use in tsplot (by condition)            \n",
    "            Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "            Condition = np.repeat(['trained','foil','control'],len(T))\n",
    "            Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "            Prob = np.hstack((T,F,C))\n",
    "            assert len(Trial)==len(Condition)\n",
    "            assert len(Sub)==len(Prob)\n",
    "            assert len(Condition)==len(Sub)\n",
    "            x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "            x = x.transpose()\n",
    "            x.columns = ['probability',lookup[this_iv],'condition','sub']\n",
    "            toop = 'condition3way'\n",
    "        else:\n",
    "            ## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "            Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "            Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "            Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "            Prob = np.hstack((DTF,DTC,DFC))        \n",
    "            assert len(Trial)==len(Condition)\n",
    "            assert len(Sub)==len(Prob)\n",
    "            assert len(Condition)==len(Sub)\n",
    "            x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "            x = x.transpose()\n",
    "            x.columns = ['probability',lookup[this_iv],'condition','sub']        \n",
    "            toop = 'difference3way'\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        ## plot it\n",
    "        sns.tsplot(data=x,\n",
    "                  time=lookup[this_iv],\n",
    "                  unit='sub',\n",
    "                  condition='condition',\n",
    "                  value='probability',\n",
    "                  ci=95)\n",
    "        if render_cond==1:\n",
    "            plt.ylim(0,2/3)\n",
    "            plt.axhline(1/3,linestyle=':',color='k')  \n",
    "            plt.legend(bbox_to_anchor=(0.8, 1.01))  \n",
    "            plt.title('Classifier evidence by condition in {}'.format(this_roi))\n",
    "            \n",
    "        else:\n",
    "            plt.ylim(-0.3,0.3)\n",
    "            plt.axhline(0,linestyle=':',color='k')  \n",
    "            plt.legend(bbox_to_anchor=(0.7, 1.01))                        \n",
    "            plt.title('Difference in classifier evidence by condition in {}'.format(this_roi))        \n",
    "        plt.xticks(np.arange(np.max(x[lookup[this_iv]].values)+1))\n",
    "        if not os.path.exists('./plots/roi/{}/{}'.format(lookup[this_iv],toop)):\n",
    "            os.makedirs('./plots/roi/{}/{}'.format(lookup[this_iv],toop))\n",
    "        plt.tight_layout()        \n",
    "        plt.savefig('./plots/roi/{}/{}/prob_timecourse_{}_by_{}.pdf'.\\\n",
    "                    format(lookup[this_iv],toop,this_roi,lookup[this_iv]))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean...Seems to be calculate of the bottom part of the graph. Look for the mean...There it is! Just swap that for max, and you have it. Though, that's the max difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALLDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get subject-level index of contrast between objects during drawing\n",
    "sub_tf = []\n",
    "sub_tc = []\n",
    "sub_fc = []\n",
    "roi = []\n",
    "\n",
    "subs = np.unique(ALLDM.subj.values)\n",
    "ivs = ['time_point'] ## other optoins 'run_num','trial_num',\n",
    "\n",
    "## do you want to render the CONDITION-wise plots -- trained vs. foil vs control\n",
    "## or the DIFFERENCE plots -- trained - foil vs foil - control?\n",
    "render_cond = 0\n",
    "\n",
    "for this_iv in ivs:\n",
    "    for this_roi in roi_list:\n",
    "\n",
    "        T = []\n",
    "        F = []\n",
    "        C = []\n",
    "        Sub = []\n",
    "        for sub in subs:\n",
    "            inds =(ALLDM['roi']==this_roi) & (ALLDM['subj']==sub) \n",
    "            t,f,c = get_prob_timecourse(this_iv,ALLDM[inds])\n",
    "            if len(T)==0:\n",
    "                T = t\n",
    "                F = f\n",
    "                C = c\n",
    "                DTF = t-f                \n",
    "                DTC = t-c\n",
    "                DFC = f-c\n",
    "            else:\n",
    "                T = np.hstack((T,t))\n",
    "                F = np.hstack((F,f))        \n",
    "                C = np.hstack((C,c)) \n",
    "                DTF = np.hstack((DTF,t-f))                \n",
    "                DTC = np.hstack((DTC,t-c))\n",
    "                DFC = np.hstack((DFC,f-c))\n",
    "            Sub.append([sub]*len(t))   \n",
    "          \n",
    "        ## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "        Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "        Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "        Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "        Prob = np.hstack((DTF,DTC,DFC))        \n",
    "        assert len(Trial)==len(Condition)\n",
    "        assert len(Sub)==len(Prob)\n",
    "        assert len(Condition)==len(Sub)\n",
    "        x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "        x = x.transpose()\n",
    "        x.columns = ['probability',lookup[this_iv],'condition','sub']\n",
    "        \n",
    "        for this_sub in subs:\n",
    "            sub_tf.append(x[(x['condition']=='trained-foil') & (x['sub']==this_sub)]['probability'].mean())\n",
    "            sub_tc.append(x[(x['condition']=='trained-control') & (x['sub']==this_sub)]['probability'].mean())  \n",
    "            sub_fc.append(x[(x['condition']=='foil-control') & (x['sub']==this_sub)]['probability'].mean()) \n",
    "            roi.append(this_roi)\n",
    "            \n",
    "## make dataframe with subject-level difference scores\n",
    "d = pd.DataFrame([sub_tf,sub_tc,sub_fc,roi])\n",
    "d = d.transpose()\n",
    "d.columns = ['trained-foil','trained-control','foil-control','roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trained-foil</th>\n",
       "      <th>trained-control</th>\n",
       "      <th>foil-control</th>\n",
       "      <th>roi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0635827</td>\n",
       "      <td>0.0198272</td>\n",
       "      <td>-0.0437555</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.0334843</td>\n",
       "      <td>-0.0310782</td>\n",
       "      <td>0.00240607</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0388928</td>\n",
       "      <td>0.0251947</td>\n",
       "      <td>-0.0136981</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.0634979</td>\n",
       "      <td>-0.0447422</td>\n",
       "      <td>0.0187557</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00228408</td>\n",
       "      <td>0.0149281</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0241669</td>\n",
       "      <td>0.0241669</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.052651</td>\n",
       "      <td>-0.0327895</td>\n",
       "      <td>0.0198615</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.128041</td>\n",
       "      <td>-0.0469741</td>\n",
       "      <td>0.0810673</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.0417419</td>\n",
       "      <td>-0.0192479</td>\n",
       "      <td>0.022494</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.00912626</td>\n",
       "      <td>-0.00912626</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0529947</td>\n",
       "      <td>0.0460332</td>\n",
       "      <td>-0.0069615</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0336945</td>\n",
       "      <td>0.0142593</td>\n",
       "      <td>-0.0194352</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.037112</td>\n",
       "      <td>-0.019811</td>\n",
       "      <td>0.0173009</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.0789616</td>\n",
       "      <td>-0.0362561</td>\n",
       "      <td>0.0427055</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.0310478</td>\n",
       "      <td>-0.0106452</td>\n",
       "      <td>0.0204026</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0130954</td>\n",
       "      <td>-0.0130954</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0433066</td>\n",
       "      <td>-0.0310209</td>\n",
       "      <td>-0.0743275</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.08595e-06</td>\n",
       "      <td>-0.00932954</td>\n",
       "      <td>-0.00933263</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.0820093</td>\n",
       "      <td>-0.0439293</td>\n",
       "      <td>0.03808</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0306697</td>\n",
       "      <td>0.0306697</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.15321</td>\n",
       "      <td>-0.0897948</td>\n",
       "      <td>0.0634154</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.0674936</td>\n",
       "      <td>-0.0437762</td>\n",
       "      <td>0.0237174</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.0387725</td>\n",
       "      <td>-0.00200867</td>\n",
       "      <td>0.0367639</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.0243296</td>\n",
       "      <td>0.0123169</td>\n",
       "      <td>0.0366464</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000108813</td>\n",
       "      <td>-0.0540337</td>\n",
       "      <td>-0.0541426</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0376018</td>\n",
       "      <td>0.0376018</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.0221948</td>\n",
       "      <td>-0.0171668</td>\n",
       "      <td>0.00502793</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0310982</td>\n",
       "      <td>0.00888467</td>\n",
       "      <td>-0.0222136</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.0152993</td>\n",
       "      <td>-0.00631323</td>\n",
       "      <td>0.00898605</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trained-foil trained-control foil-control roi\n",
       "0     0.0635827       0.0198272   -0.0437555  V1\n",
       "1    -0.0334843      -0.0310782   0.00240607  V1\n",
       "2     0.0388928       0.0251947   -0.0136981  V1\n",
       "3    -0.0634979      -0.0447422    0.0187557  V1\n",
       "4    0.00228408       0.0149281     0.012644  V1\n",
       "5             0       0.0241669    0.0241669  V1\n",
       "6     -0.052651      -0.0327895    0.0198615  V1\n",
       "7     -0.128041      -0.0469741    0.0810673  V1\n",
       "8    -0.0417419      -0.0192479     0.022494  V1\n",
       "9             0        0.012598     0.012598  V1\n",
       "10            0     -0.00912626  -0.00912626  V1\n",
       "11    0.0529947       0.0460332   -0.0069615  V1\n",
       "12    0.0336945       0.0142593   -0.0194352  V1\n",
       "13    -0.037112       -0.019811    0.0173009  V1\n",
       "14   -0.0789616      -0.0362561    0.0427055  V1\n",
       "15   -0.0310478      -0.0106452    0.0204026  V1\n",
       "16            0      -0.0130954   -0.0130954  V1\n",
       "17    0.0433066      -0.0310209   -0.0743275  V1\n",
       "18  3.08595e-06     -0.00932954  -0.00933263  V1\n",
       "19   -0.0820093      -0.0439293      0.03808  V1\n",
       "20            0       0.0306697    0.0306697  V1\n",
       "21     -0.15321      -0.0897948    0.0634154  V1\n",
       "22   -0.0674936      -0.0437762    0.0237174  V1\n",
       "23   -0.0387725     -0.00200867    0.0367639  V1\n",
       "24   -0.0243296       0.0123169    0.0366464  V1\n",
       "25  0.000108813      -0.0540337   -0.0541426  V1\n",
       "26            0       0.0376018    0.0376018  V1\n",
       "27   -0.0221948      -0.0171668   0.00502793  V1\n",
       "28    0.0310982      0.00888467   -0.0222136  V1\n",
       "29   -0.0152993     -0.00631323   0.00898605  V1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['roi']=='V1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just tests if any of these differences are reliably different from zero. Some are, even taking into account multiple measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = 2.768736123485365, p = 0.009554778830532455\n",
      "ROI = V2 | t = 2.770583015369253, p = 0.0095119210686561\n",
      "ROI = LOC | t = 4.596186237182268, p = 7.268034217384911e-05\n",
      "ROI = IT | t = 0.32746038433578734, p = 0.7455939050323976\n",
      "ROI = fusiform | t = 2.1010699856853625, p = 0.04414153968778241\n",
      "ROI = parahippo | t = 0.5293455190029417, p = 0.6004613045029474\n",
      "ROI = PRC | t = 0.016453483562522103, p = 0.986981551664948\n",
      "ROI = ento | t = -0.6840172702888475, p = 0.49921431079537637\n",
      "ROI = hipp | t = -1.0407359788537531, p = 0.3063133092292496\n",
      "ROI = mOFC | t = 0.5196500057946879, p = 0.6071208489160427\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['trained-foil']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = 1.6636248275724903, p = 0.10660079640244327\n",
      "ROI = V2 | t = 2.2139350332488115, p = 0.03457771253468806\n",
      "ROI = LOC | t = 2.415038847515531, p = 0.02203647713504597\n",
      "ROI = IT | t = -0.3861479810294371, p = 0.7021121715021897\n",
      "ROI = fusiform | t = 1.6227296794866946, p = 0.11511072025005005\n",
      "ROI = parahippo | t = -0.27083607057368286, p = 0.7883718940652598\n",
      "ROI = PRC | t = 1.2985961595839934, p = 0.20397641602298494\n",
      "ROI = ento | t = 0.6538942794524435, p = 0.5181612790511406\n",
      "ROI = hipp | t = -0.4834968962225369, p = 0.6322533131124466\n",
      "ROI = mOFC | t = 0.4752748443740968, p = 0.6380333575379928\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['trained-control']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = -2.3466838298413037, p = 0.0257378940389738\n",
      "ROI = V2 | t = -2.5411352786501786, p = 0.016457522733690673\n",
      "ROI = LOC | t = -3.1123034695161897, p = 0.004055278623925445\n",
      "ROI = IT | t = -0.8491410233102968, p = 0.4025281934473953\n",
      "ROI = fusiform | t = -1.4784845160787863, p = 0.14970188244735996\n",
      "ROI = parahippo | t = -0.9710093202551024, p = 0.33930791332804766\n",
      "ROI = PRC | t = 0.9153607662983471, p = 0.36730173339353445\n",
      "ROI = ento | t = 1.3457073383157303, p = 0.1884798586934702\n",
      "ROI = hipp | t = 0.8915989428781647, p = 0.37970360289586\n",
      "ROI = mOFC | t = 0.1614653384488601, p = 0.8728094242188358\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['foil-control']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple graph. X is foil-control, y is prepostdiff scores. Plot points, correlation, etc. All I'm changing is x-axis, and I'm doing it above. Cool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepost = pd.read_csv('neural_changes_by_surfroi_and_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1\n",
      "V2\n",
      "LOC\n",
      "IT\n",
      "fusiform\n",
      "parahippo\n",
      "PRC\n",
      "ento\n",
      "hipp\n",
      "mOFC\n"
     ]
    }
   ],
   "source": [
    "## make dataframe to relate drawing contrast to recognition differentiation\n",
    "roi_list = ['V1', 'V2', 'LOC', 'IT', 'fusiform', 'parahippo', 'PRC', 'ento','hipp', 'mOFC']\n",
    "this_roi = 'hipp'\n",
    "\n",
    "for this_roi in roi_list:\n",
    "    print(this_roi)\n",
    "    draw = d[d['roi']==this_roi]['trained-control'].values - d[d['roi']==this_roi]['foil-control'].values\n",
    "    recog = prepost['tradiff_{}'.format(this_roi)].values-prepost['condiff_{}'.format(this_roi)].values\n",
    "\n",
    "    z = pd.DataFrame([draw,recog])\n",
    "    z = z.transpose()\n",
    "    z.columns=['draw','recog']\n",
    "\n",
    "    ## plot \n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    sns.set_context('poster')\n",
    "    sns.regplot(x=\"draw\",\n",
    "                y =\"recog\",\n",
    "                data=z)\n",
    "    r,p = stats.pearsonr(draw,recog)\n",
    "    plt.title('ROI: {}  r={}  p={}'.format(this_roi,np.round(r,3),np.round(p,3)))\n",
    "    if not os.path.exists('./plots/roi/drawrecog'):\n",
    "        os.makedirs('./plots/roi/drawrecog')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/roi/drawrecog/draw_recog_scatter_{}.pdf'.format(this_roi))\n",
    "    plt.close(fig)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relate neural to vgg drawing time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./logistic_timeseries_drawing_vgg.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-171bafc1696e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logistic_timeseries_drawing_vgg.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneural_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logistic_timeseries_drawing_neural.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./logistic_timeseries_drawing_vgg.csv' does not exist"
     ]
    }
   ],
   "source": [
    "vgg_ts = pd.read_csv('./logistic_timeseries_drawing_vgg.csv')\n",
    "neural_ts = pd.read_csv('./logistic_timeseries_drawing_neural.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_ts = vgg_ts.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)\n",
    "vgg_ts.wID = [i.split('_')[0] for i in vgg_ts.wID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>wID</th>\n",
       "      <th>viewpoint</th>\n",
       "      <th>trial</th>\n",
       "      <th>trialDuration</th>\n",
       "      <th>target</th>\n",
       "      <th>competitor</th>\n",
       "      <th>numSketch</th>\n",
       "      <th>bed</th>\n",
       "      <th>bench</th>\n",
       "      <th>chair</th>\n",
       "      <th>table</th>\n",
       "      <th>curr_winner</th>\n",
       "      <th>tc_pair</th>\n",
       "      <th>trialID</th>\n",
       "      <th>run</th>\n",
       "      <th>target_val</th>\n",
       "      <th>competitor_val</th>\n",
       "      <th>control_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>chair</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>0.013931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934225</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>bed</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.467345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.970770</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970770</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.013258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.967674</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967674</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.015431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      wID  viewpoint  trial  trialDuration target competitor  \\\n",
       "0      0  0119174         20    320       39.00144  bench      chair   \n",
       "1      0  0119174         20    320       39.00144  bench      chair   \n",
       "2      0  0119174         20    320       39.00144  bench      chair   \n",
       "3      0  0119174         20    320       39.00144  bench      chair   \n",
       "4      0  0119174         20    320       39.00144  bench      chair   \n",
       "\n",
       "   numSketch       bed     bench     chair     table curr_winner      tc_pair  \\\n",
       "0          0  0.026822  0.055895  0.916243  0.001040       chair  bench/chair   \n",
       "1          1  0.934225  0.026225  0.039085  0.000465         bed  bench/chair   \n",
       "2          2  0.006035  0.970770  0.002714  0.020481       bench  bench/chair   \n",
       "3          3  0.009842  0.978927  0.003784  0.007447       bench  bench/chair   \n",
       "4          4  0.019298  0.967674  0.001463  0.011564       bench  bench/chair   \n",
       "\n",
       "                   trialID  run  target_val  competitor_val  control_val  \n",
       "0  0119174_neurosketch_320  1.0    0.055895        0.916243     0.013931  \n",
       "1  0119174_neurosketch_320  1.0    0.026225        0.039085     0.467345  \n",
       "2  0119174_neurosketch_320  1.0    0.970770        0.002714     0.013258  \n",
       "3  0119174_neurosketch_320  1.0    0.978927        0.003784     0.008644  \n",
       "4  0119174_neurosketch_320  1.0    0.967674        0.001463     0.015431  "
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2000489150>]"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEECAYAAAAlEzNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZE0pyAQEUlzCgIRkTSnIBARSXP/P9WPn1x+dViFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ff3e34f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vgg_ts.groupby('numSketch')['target_val'].mean())\n",
    "plt.plot(vgg_ts.groupby('numSketch')['competitor_val'].mean(),color='green')\n",
    "plt.plot(vgg_ts.groupby('numSketch')['control_val'].mean(),color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plotting helper\n",
    "def get_vgg_timecourse(iv,DM):\n",
    "    trained_objs = np.unique(DM.target.values)\n",
    "    control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "    t1 = trained_objs[0]\n",
    "    t2 = trained_objs[1]\n",
    "    c1 = control_objs[0]\n",
    "    c2 = control_objs[1]\n",
    "    target = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t1)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t2)].mean().values)).mean(0) ## target timecourse\n",
    "    foil = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t2)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t1)].mean().values)).mean(0) ## foil timecourse\n",
    "    control = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t1].groupby(iv)['{}_prob'.format(c2)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c2)].mean().values)).mean(0) ## control timecourse\n",
    "    \n",
    "    return target, foil, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = []\n",
    "F = []\n",
    "C = []\n",
    "Sub = []\n",
    "for sub in subs:\n",
    "    inds =(vgg_ts['wID']==sub) \n",
    "    t,f,c = get_prob_timecourse(this_iv,vgg_ts[inds])\n",
    "    \n",
    "    \n",
    "    if len(T)==0:\n",
    "        T = t\n",
    "        F = f\n",
    "        C = c\n",
    "        DTF = t-f                \n",
    "        DTC = t-c\n",
    "        DFC = f-c\n",
    "    else:\n",
    "        T = np.hstack((T,t))\n",
    "        F = np.hstack((F,f))        \n",
    "        C = np.hstack((C,c)) \n",
    "        DTF = np.hstack((DTF,t-f))                \n",
    "        DTC = np.hstack((DTC,t-c))\n",
    "        DFC = np.hstack((DFC,f-c))\n",
    "    Sub.append([sub]*len(t))   \n",
    "\n",
    "## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "Prob = np.hstack((DTF,DTC,DFC))        \n",
    "assert len(Trial)==len(Condition)\n",
    "assert len(Sub)==len(Prob)\n",
    "assert len(Condition)==len(Sub)\n",
    "x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "x = x.transpose()\n",
    "x.columns = ['probability',lookup[this_iv],'condition','sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0110171', '0110172', '0111171', '0112171', '0112172', '0112173',\n",
       "       '0113171', '0115174', '0117171', '0118171', '0118172', '0119171',\n",
       "       '0119172', '0119173', '0119174', '0120171', '0120172', '0120173',\n",
       "       '0123171', '0123173', '0124171', '0125171', '0125172', '1121161',\n",
       "       '1130161', '1202161', '1203161', '1206161', '1206162', '1206163',\n",
       "       '1207162'], dtype=object)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0110171_neurosketch', '0110172_neurosketch', '0111171_neurosketch', '0112171_neurosketch', '0112172_neurosketch', '0112173_neurosketch', '0113171_neurosketch', '0115174_neurosketch', '0117171_neurosketch', '0118171_neurosketch', '0118172_neurosketch', '0119171_neurosketch', '0119172_neurosketch', '0119173_neurosketch', '0119174_neurosketch', '0120171_neurosketch', '0120172_neurosketch', '0120173_neurosketch', '0123171_neurosketch', '0123173_neurosketch', '0124171_neurosketch', '0125171_neurosketch', '0125172_neurosketch', '1121161_neurosketch', '1130161_neurosketch', '1202161_neurosketch', '1203161_neurosketch', '1206161_neurosketch', '1206162_neurosketch', '1206163_neurosketch', '1207162_neurosketch']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(['1121161_neurosketch', '1130161_neurosketch', '1202161_neurosketch', '1203161_neurosketch', '1206161_neurosketch', '1206162_neurosketch', '1206163_neurosketch', '1207162_neurosketch', '0110171_neurosketch', \n",
    "                '0110172_neurosketch', '0111171_neurosketch', '0112171_neurosketch', '0112172_neurosketch', '0112173_neurosketch', '0113171_neurosketch', '0115174_neurosketch', '0117171_neurosketch', '0118171_neurosketch', \n",
    "                '0118172_neurosketch', '0119171_neurosketch', '0119172_neurosketch', '0119173_neurosketch', '0119174_neurosketch', '0120171_neurosketch', '0120172_neurosketch', '0120173_neurosketch', '0123171_neurosketch', \n",
    "                '0123173_neurosketch', '0124171_neurosketch', '0125171_neurosketch', '0125172_neurosketch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
