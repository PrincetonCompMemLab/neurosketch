{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import linear_model, datasets, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.misc import imread, imresize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotta change the path and roi_list here to include the new ROIs I've made.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define path to input datasets (tidy format)\n",
    "path_to_recog = '/home/jefan/neurosketch_compmem/neurosketch_voxelmat_freesurfer_recog'#'/home/jgunn/neurosketch/recmatrices' #\n",
    "path_to_draw = '/home/jefan/neurosketch_compmem/neurosketch_voxelmat_freesurfer_drawing' #'/home/jgunn/neurosketch/drawmatrices' #\n",
    "roi_list = np.array(['V1','V2','LOC','IT','fusiform','parahippo', 'PRC', 'ento','hipp','mOFC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get raw file list for recognition runs\n",
    "RECOG_METAS = sorted([i for i in os.listdir(path_to_recog) if i.split('.')[-1]=='csv'])\n",
    "RECOG_FEATS = sorted([i for i in os.listdir(path_to_recog) if i.split('.')[-1]=='npy'])\n",
    "RECOG_SUBS = np.array([i.split('_')[0] for i in RECOG_FEATS])\n",
    "\n",
    "recog_sub_list = np.unique(RECOG_SUBS)\n",
    "\n",
    "def preprocess_recog(RECOG_METAS, RECOG_FEATS):\n",
    "    M = [i for i in RECOG_METAS if len(i.split('.')[0].split('_'))==4]\n",
    "    F = [i for i in RECOG_FEATS if len(i.split('.')[0].split('_'))==4]\n",
    "    return M,F\n",
    "\n",
    "RECOG_METAS, RECOG_FEATS = preprocess_recog(RECOG_METAS, RECOG_FEATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get raw file list for drawing runs\n",
    "DRAW_METAS = sorted([i for i in os.listdir(path_to_draw) if i.split('.')[-1]=='csv'])\n",
    "DRAW_FEATS = sorted([i for i in os.listdir(path_to_draw) if i.split('.')[-1]=='npy'])\n",
    "DRAW_SUBS = np.array([i.split('_')[0] for i in DRAW_FEATS])\n",
    "draw_sub_list = np.unique(DRAW_SUBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subs: 31\n"
     ]
    }
   ],
   "source": [
    "## get subject ID's that have complete datasets from all phases of experiment\n",
    "sub_list = np.intersect1d(recog_sub_list,draw_sub_list)\n",
    "print('Number of subs: {}'.format(len(sub_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## filter file list so only contains the sessions that have full datasets\n",
    "def extract_good_sessions(DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS):\n",
    "    _DRAW_METAS = [i for i in DRAW_METAS if i.split('_')[1] in sub_list]\n",
    "    _DRAW_FEATS = [i for i in DRAW_FEATS if i.split('_')[0] in sub_list]\n",
    "    _RECOG_METAS = [i for i in RECOG_METAS if i.split('_')[1] in sub_list]\n",
    "    _RECOG_FEATS = [i for i in RECOG_FEATS if i.split('_')[0] in sub_list]\n",
    "    return _DRAW_METAS, _DRAW_FEATS, _RECOG_METAS, _RECOG_FEATS\n",
    "\n",
    "DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS =  \\\n",
    "extract_good_sessions(DRAW_METAS,DRAW_FEATS,RECOG_METAS,RECOG_FEATS)\n",
    "\n",
    "RECOG_SUBS = np.array([i.split('_')[0]+'_neurosketch' for i in RECOG_FEATS])\n",
    "RECOG_ROIS = np.array([i.split('_')[1] for i in RECOG_FEATS])\n",
    "\n",
    "DRAW_SUBS = np.array([i.split('_')[0]+'_neurosketch' for i in DRAW_FEATS])\n",
    "DRAW_ROIS = np.array([i.split('_')[1] for i in DRAW_FEATS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well do we do at classifying the target when we train on recognition patterns only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Helper data loader functions\n",
    "def load_draw_meta(this_sub):\n",
    "    this_file = 'metadata_{}_drawing.csv'.format(this_sub)\n",
    "    x = pd.read_csv(os.path.join(path_to_draw,this_file))\n",
    "    x = x.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "    x['trial_num'] = np.repeat(np.arange(40),23)        \n",
    "    return x\n",
    "    \n",
    "def load_draw_feats(this_sub,this_roi):\n",
    "    this_file = '{}_{}_featurematrix.npy'.format(this_sub,this_roi)\n",
    "    y = np.load(os.path.join(path_to_draw,this_file))\n",
    "    y = y.transpose()\n",
    "    return y\n",
    "\n",
    "def load_draw_data(this_sub,this_roi):\n",
    "    x = load_draw_meta(this_sub)\n",
    "    y = load_draw_feats(this_sub,this_roi)\n",
    "    assert y.shape[0] == x.shape[0]    \n",
    "    return x,y\n",
    "\n",
    "def load_recog_meta(this_sub,this_roi,this_phase):\n",
    "    this_file = 'metadata_{}_{}_{}.csv'.format(this_sub,this_roi,this_phase)\n",
    "    x = pd.read_csv(os.path.join(path_to_recog,this_file))\n",
    "    x = x.drop(['Unnamed: 0'], axis=1)\n",
    "    return x\n",
    "    \n",
    "def load_recog_feats(this_sub,this_roi,this_phase):\n",
    "    this_file = '{}_{}_{}_featurematrix.npy'.format(this_sub,this_roi,this_phase)\n",
    "    y = np.load(os.path.join(path_to_recog,this_file))\n",
    "    y = y.transpose()\n",
    "    return y    \n",
    "\n",
    "def load_recog_data(this_sub,this_roi,this_phase):\n",
    "    x = load_recog_meta(this_sub,this_roi,this_phase)\n",
    "    y = load_recog_feats(this_sub,this_roi,this_phase)\n",
    "    assert y.shape[0] == x.shape[0]    \n",
    "    return x,y\n",
    "\n",
    "# z-score normalization to de-mean & standardize variances within-voxel \n",
    "def normalize(X):\n",
    "    X = X - X.mean(0)\n",
    "    X = X / np.maximum(X.std(0), 1e-5)\n",
    "    return X\n",
    "\n",
    "## plotting helper\n",
    "def get_prob_timecourse(iv,DM):\n",
    "    trained_objs = np.unique(DM.label.values)\n",
    "    control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "    t1 = trained_objs[0]\n",
    "    t2 = trained_objs[1]\n",
    "    c1 = control_objs[0]\n",
    "    c2 = control_objs[1]\n",
    "    target = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t1)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t2)].mean().values)).mean(0) ## target timecourse\n",
    "    foil = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t2)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t1)].mean().values)).mean(0) ## foil timecourse\n",
    "    control = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t1].groupby(iv)['{}_prob'.format(c2)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c2)].mean().values)).mean(0) ## control timecourse\n",
    "    \n",
    "    return target, foil, control\n",
    "     \n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## general plotting params\n",
    "sns.set_context('poster')\n",
    "colors = sns.color_palette(\"cubehelix\", 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## example subject, roi, and phase (localizer is default)\n",
    "this_sub = sub_list[0]\n",
    "this_roi = roi_list[2] ## order is: ['V1','V2','LOC','IT','fusiform','parahippo', 'PRC', 'ento','hipp','mOFC']\n",
    "this_phase = '12' ## options are '12', '34', '56'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so to do my thing, I need to change the code so that it 1) subsets based on label instead of timepoint (one control label, and then another) 2) trains two classifiers, one for each subset, 3) take the mean of prediction probabilities, finally storing these in each <obj>_prob thing. Pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "V2\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "LOC\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "IT\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "fusiform\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "parahippo\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "PRC\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "ento\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "hipp\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n",
      "mOFC\n",
      "0110171\n",
      "0110172\n",
      "0111171\n",
      "0112171\n",
      "0112172\n",
      "0112173\n",
      "0113171\n",
      "0115174\n",
      "0117171\n",
      "0118171\n",
      "0118172\n",
      "0119171\n",
      "0119172\n",
      "0119173\n",
      "0119174\n",
      "0120171\n",
      "0120172\n",
      "0120173\n",
      "0123171\n",
      "0123173\n",
      "0124171\n",
      "0125171\n",
      "0125172\n",
      "1121161\n",
      "1130161\n",
      "1202161\n",
      "1203161\n",
      "1206161\n",
      "1206162\n",
      "1206163\n",
      "1207162\n"
     ]
    }
   ],
   "source": [
    "ALLDM = []\n",
    "\n",
    "## loop through all subjects and rois\n",
    "Acc = []\n",
    "for this_roi in roi_list:\n",
    "    print (this_roi)\n",
    "    acc = []\n",
    "    for this_sub in sub_list:\n",
    "        print(this_sub)\n",
    "        ## load subject data in\n",
    "        RM12, RF12 = load_recog_data(this_sub,this_roi,'12')\n",
    "        RM34, RF34 = load_recog_data(this_sub,this_roi,'34')        \n",
    "        RM = pd.concat([RM12,RM34])\n",
    "        RF = np.vstack((RF12,RF34))        \n",
    "        DM, DF = load_draw_data(this_sub,this_roi)\n",
    "        assert RF.shape[1]==DF.shape[1] ## that number of voxels is identical\n",
    "        \n",
    "        # identify control objects; \n",
    "        # we wil train one classifier with \n",
    "        trained_objs = np.unique(DM.label.values)\n",
    "        control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        probs = []\n",
    "        \n",
    "        for ctrl in control_objs:\n",
    "            \n",
    "            inds = RM.label != ctrl\n",
    "            _RM = RM[inds]\n",
    "\n",
    "            ## normalize voxels within task\n",
    "            normalize_on = 1\n",
    "            if normalize_on:\n",
    "                _RF = normalize(RF[inds,:])\n",
    "                _DF = normalize(DF)\n",
    "            else:\n",
    "                _RF = RF[inds,:]\n",
    "                _DF = DF\n",
    "\n",
    "            # single train/test split\n",
    "            X_train = _RF # recognition run feature set\n",
    "            y_train = _RM.label.values # list of labels for the training set\n",
    "\n",
    "            ## subset timepoints?\n",
    "            inds = DM.time_point>0 # all timepoints are > 0, so no subsetting happens\n",
    "            _DF = _DF[inds,:]\n",
    "            DM = DM[inds]\n",
    "\n",
    "            X_test = _DF\n",
    "            y_test = DM.label.values\n",
    "            # clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "            clf = linear_model.LogisticRegression(penalty='l2',C=1).fit(X_train, y_train)    \n",
    "\n",
    "            ## add prediction probabilities to metadata matrix\n",
    "            cats = clf.classes_\n",
    "            probs.append(clf.predict_proba(X_test))\n",
    "\n",
    "        DM['bed_prob'] = (probs[0][:,0] + probs[1][:,0])/2.0\n",
    "        DM['bench_prob'] = (probs[0][:,1] + probs[1][:,1])/2.0\n",
    "        DM['chair_prob'] = (probs[0][:,2] + probs[1][:,2])/2.0\n",
    "        DM['table_prob'] = (probs[0][:,2] + probs[1][:,2])/2.0\n",
    "        \n",
    "        DM['subj'] = np.repeat(this_sub,DM.shape[0])\n",
    "        DM['roi'] = np.repeat(this_roi,DM.shape[0])\n",
    "        \n",
    "        if len(ALLDM)==0:\n",
    "            ALLDM = DM\n",
    "        else:\n",
    "            ALLDM = pd.concat([ALLDM,DM],ignore_index=True)\n",
    "\n",
    "        ## plot probability timecourse\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        iv = 'run_num'\n",
    "        t,f,c = get_prob_timecourse(iv,DM)\n",
    "        plt.plot(t,color=colors[0],label='target')\n",
    "        plt.plot(f,color=colors[1],label='foil')\n",
    "        plt.plot(c,color=colors[2],label='control')\n",
    "        plt.legend(bbox_to_anchor=(1.45, 1.01))\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel(iv)\n",
    "        plt.ylabel('probability')\n",
    "        if not os.path.exists('./plots/subj'):\n",
    "            os.makedirs('./plots/subj')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./plots/subj/{}_{}_prob_{}.pdf'.format(iv.split('_')[0],this_roi,this_sub))\n",
    "        plt.close(fig)\n",
    "        acc.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    Acc.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #trained_objs = np.unique(DM.label.values)\n",
    "        #control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "        #inds = DM.label != 'chair' # all timepoints are > 0, so no subsetting happens\n",
    "        \n",
    "#np.shape(probs)\n",
    "#(probs[0][:,0] + probs[1][:,0])/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALLDM = ALLDM.drop(['Unnamed: 0.1.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>LOC</th>\n",
       "      <th>IT</th>\n",
       "      <th>fusiform</th>\n",
       "      <th>parahippo</th>\n",
       "      <th>PRC</th>\n",
       "      <th>ento</th>\n",
       "      <th>hipp</th>\n",
       "      <th>mOFC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.343478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305435</td>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.340217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.307609</td>\n",
       "      <td>0.351087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.272826</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.314130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.336957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.476087</td>\n",
       "      <td>0.484783</td>\n",
       "      <td>0.444565</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.314130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.406522</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.360870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.526087</td>\n",
       "      <td>0.454348</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.372826</td>\n",
       "      <td>0.338043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.394565</td>\n",
       "      <td>0.398913</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.370652</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.336957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.397826</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.303261</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.346739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.527174</td>\n",
       "      <td>0.533696</td>\n",
       "      <td>0.420652</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.384783</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.372826</td>\n",
       "      <td>0.421739</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.318478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.415217</td>\n",
       "      <td>0.459783</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.340217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.285870</td>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.381522</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.285870</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.353261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.388043</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.341304</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.338043</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.294565</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.330435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.511957</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.267391</td>\n",
       "      <td>0.410870</td>\n",
       "      <td>0.320652</td>\n",
       "      <td>0.305435</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.303261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.353261</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.346739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.334783</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.319565</td>\n",
       "      <td>0.358696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.460870</td>\n",
       "      <td>0.491304</td>\n",
       "      <td>0.383696</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.286957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.444565</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.416304</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.323913</td>\n",
       "      <td>0.318478</td>\n",
       "      <td>0.340217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.463043</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.296739</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.363043</td>\n",
       "      <td>0.315217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.459783</td>\n",
       "      <td>0.451087</td>\n",
       "      <td>0.357609</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.301087</td>\n",
       "      <td>0.339130</td>\n",
       "      <td>0.342391</td>\n",
       "      <td>0.301087</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.317391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.292391</td>\n",
       "      <td>0.273913</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>0.280435</td>\n",
       "      <td>0.360870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.346739</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.322826</td>\n",
       "      <td>0.292391</td>\n",
       "      <td>0.316304</td>\n",
       "      <td>0.309783</td>\n",
       "      <td>0.367391</td>\n",
       "      <td>0.376087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.378261</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.406522</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.352174</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.308696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.515217</td>\n",
       "      <td>0.409783</td>\n",
       "      <td>0.345652</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.302174</td>\n",
       "      <td>0.329348</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.303261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.289130</td>\n",
       "      <td>0.311957</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.371739</td>\n",
       "      <td>0.340217</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.371739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.396739</td>\n",
       "      <td>0.431522</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.327174</td>\n",
       "      <td>0.356522</td>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.343478</td>\n",
       "      <td>0.332609</td>\n",
       "      <td>0.290217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.335870</td>\n",
       "      <td>0.354348</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.359783</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.344565</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>0.302174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.379348</td>\n",
       "      <td>0.377174</td>\n",
       "      <td>0.283696</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.310870</td>\n",
       "      <td>0.351087</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.285870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2       LOC        IT  fusiform  parahippo       PRC  \\\n",
       "0   0.333696  0.369565  0.368478  0.343478  0.304348   0.342391  0.357609   \n",
       "1   0.305435  0.316304  0.364130  0.339130  0.358696   0.334783  0.330435   \n",
       "2   0.332609  0.329348  0.297826  0.382609  0.380435   0.320652  0.354348   \n",
       "3   0.408696  0.391304  0.272826  0.338043  0.354348   0.343478  0.326087   \n",
       "4   0.319565  0.382609  0.338043  0.336957  0.326087   0.342391  0.348913   \n",
       "5   0.476087  0.484783  0.444565  0.338043  0.336957   0.339130  0.319565   \n",
       "6   0.391304  0.378261  0.406522  0.342391  0.331522   0.302174  0.278261   \n",
       "7   0.526087  0.454348  0.426087  0.338043  0.341304   0.321739  0.332609   \n",
       "8   0.394565  0.398913  0.369565  0.370652  0.339130   0.346739  0.342391   \n",
       "9   0.397826  0.356522  0.319565  0.311957  0.303261   0.332609  0.334783   \n",
       "10  0.527174  0.533696  0.420652  0.365217  0.384783   0.341304  0.345652   \n",
       "11  0.369565  0.372826  0.421739  0.330435  0.354348   0.300000  0.352174   \n",
       "12  0.415217  0.459783  0.333696  0.338043  0.348913   0.311957  0.320652   \n",
       "13  0.311957  0.285870  0.334783  0.342391  0.381522   0.367391  0.346739   \n",
       "14  0.388043  0.382609  0.341304  0.320652  0.328261   0.339130  0.320652   \n",
       "15  0.338043  0.393478  0.336957  0.329348  0.368478   0.308696  0.331522   \n",
       "16  0.443478  0.511957  0.417391  0.267391  0.410870   0.320652  0.305435   \n",
       "17  0.316304  0.339130  0.352174  0.304348  0.353261   0.330435  0.353261   \n",
       "18  0.334783  0.358696  0.382609  0.351087  0.402174   0.328261  0.329348   \n",
       "19  0.460870  0.491304  0.383696  0.330435  0.335870   0.328261  0.354348   \n",
       "20  0.444565  0.450000  0.416304  0.345652  0.352174   0.331522  0.342391   \n",
       "21  0.475000  0.463043  0.331522  0.288043  0.354348   0.296739  0.339130   \n",
       "22  0.459783  0.451087  0.357609  0.329348  0.301087   0.339130  0.342391   \n",
       "23  0.304348  0.325000  0.328261  0.292391  0.273913   0.327174  0.313043   \n",
       "24  0.346739  0.365217  0.291304  0.286957  0.322826   0.292391  0.316304   \n",
       "25  0.378261  0.368478  0.406522  0.306522  0.352174   0.308696  0.329348   \n",
       "26  0.408696  0.515217  0.409783  0.345652  0.347826   0.343478  0.302174   \n",
       "27  0.289130  0.311957  0.330435  0.269565  0.371739   0.340217  0.330435   \n",
       "28  0.396739  0.431522  0.368478  0.327174  0.356522   0.335870  0.368478   \n",
       "29  0.335870  0.354348  0.333696  0.368478  0.375000   0.359783  0.369565   \n",
       "30  0.379348  0.377174  0.283696  0.358696  0.310870   0.351087  0.364130   \n",
       "\n",
       "        ento      hipp      mOFC  \n",
       "0   0.348913  0.335870  0.343478  \n",
       "1   0.346739  0.331522  0.340217  \n",
       "2   0.333696  0.307609  0.351087  \n",
       "3   0.328261  0.313043  0.314130  \n",
       "4   0.354348  0.311957  0.336957  \n",
       "5   0.331522  0.326087  0.314130  \n",
       "6   0.343478  0.336957  0.360870  \n",
       "7   0.326087  0.372826  0.338043  \n",
       "8   0.341304  0.356522  0.336957  \n",
       "9   0.361957  0.313043  0.346739  \n",
       "10  0.342391  0.375000  0.325000  \n",
       "11  0.356522  0.336957  0.318478  \n",
       "12  0.297826  0.332609  0.340217  \n",
       "13  0.285870  0.322826  0.353261  \n",
       "14  0.321739  0.341304  0.300000  \n",
       "15  0.294565  0.345652  0.330435  \n",
       "16  0.318478  0.344565  0.303261  \n",
       "17  0.311957  0.336957  0.346739  \n",
       "18  0.326087  0.319565  0.358696  \n",
       "19  0.333696  0.326087  0.286957  \n",
       "20  0.323913  0.318478  0.340217  \n",
       "21  0.322826  0.363043  0.315217  \n",
       "22  0.301087  0.352174  0.317391  \n",
       "23  0.361957  0.280435  0.360870  \n",
       "24  0.309783  0.367391  0.376087  \n",
       "25  0.356522  0.315217  0.308696  \n",
       "26  0.329348  0.351087  0.303261  \n",
       "27  0.332609  0.304348  0.371739  \n",
       "28  0.343478  0.332609  0.290217  \n",
       "29  0.344565  0.321739  0.302174  \n",
       "30  0.355435  0.313043  0.285870  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Acc = np.array(Acc) # 10 ROIs, 31 subjects. A score representing classification performance for that subject/roi pair\n",
    "x = pd.DataFrame(Acc.transpose())\n",
    "x.columns = roi_list\n",
    "\n",
    "x # x is acc formatted into a ROI x subject dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.45)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAEOCAYAAAC5AM74AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHMdxQ45XQjmO4ziO47gh9z/S7lehLsR6bQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9840c0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_context('talk')\n",
    "plt.figure(figsize=(11,4))\n",
    "sns.barplot(data=x,palette='husl',ci=95) # very amazing thing sns must be!\n",
    "plt.axhline(1/3,linestyle=':',color='k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test on drawing runs; train on recognition runs')\n",
    "plt.ylim(0,0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALLDM.to_csv('./logistic_timeseries_neural_vgg.csv') ## train recog, test drawing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TR_num</th>\n",
       "      <th>bed_prob</th>\n",
       "      <th>bench_prob</th>\n",
       "      <th>chair_prob</th>\n",
       "      <th>label</th>\n",
       "      <th>roi</th>\n",
       "      <th>run_num</th>\n",
       "      <th>subj</th>\n",
       "      <th>table_prob</th>\n",
       "      <th>time_point</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.849860</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>bed</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0110171</td>\n",
       "      <td>0.138852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.969912</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>bed</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0110171</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.371163</td>\n",
       "      <td>0.179743</td>\n",
       "      <td>0.449093</td>\n",
       "      <td>bed</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0110171</td>\n",
       "      <td>0.449093</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0.248547</td>\n",
       "      <td>0.407146</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>bed</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0110171</td>\n",
       "      <td>0.344307</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>0.540043</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.455972</td>\n",
       "      <td>bed</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0110171</td>\n",
       "      <td>0.455972</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TR_num  bed_prob  bench_prob  chair_prob label roi  run_num     subj  \\\n",
       "0      10  0.849860    0.011288    0.138852   bed  V1        1  0110171   \n",
       "1      11  0.969912    0.008953    0.021136   bed  V1        1  0110171   \n",
       "2      12  0.371163    0.179743    0.449093   bed  V1        1  0110171   \n",
       "3      13  0.248547    0.407146    0.344307   bed  V1        1  0110171   \n",
       "4      14  0.540043    0.003985    0.455972   bed  V1        1  0110171   \n",
       "\n",
       "   table_prob  time_point  trial_num  \n",
       "0    0.138852           1          0  \n",
       "1    0.021136           2          0  \n",
       "2    0.449093           3          0  \n",
       "3    0.344307           4          0  \n",
       "4    0.455972           5          0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALLDM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how does this code work?\n",
    "\n",
    "`lookup` is weird.\n",
    "\n",
    "Scans through `['run_num','trial_num','time_point']`, then ROIs, then subjects too.\n",
    "\n",
    "Now what is T F C? Trained, Foil Control. uses the helper function on a subset of ALLDMS that focuses on this roi and this subject. How does that function work?\n",
    "seems to first take the unique set of labels..? How is he telling between trained and untrained objs? Seems there's an ordering, and unique preserves it? Drawing trials will always only be the trained ones. Duh. Control is never the accurate thing, but it always gets assigned a score. This'll make the other coding problem easier.\n",
    "\n",
    "So it isolates by roi, subject and object, groups iteratively by ['run_num','trial_num','time_point'] and takes the mean across those groupings for each object category. So I get 4 means for 4 runs, and 3 sets of means, one for each condition. Need more focus on how foil is calculated, though. It's just t2, that's all.\n",
    "\n",
    "So let's start over knowing what I know. Scanning through the units I want my timecourse taken over (runs, trials, timepoints), then over ROIs, then over subjects. On the subject level, I'm getting the probability time course over the current iv for trained, foil, and control. For each sub, store said timecourse in T, F, or C. DTF DTC and DFC are the differences from D to F, etc. If render cond is positive, the plot will be trained, foil, control. Otherwise, it's DTF and so forth. \n",
    "\n",
    "Then generate graph for this. DOn't know if I need to know the details. I'm looking to set up the final graphs. Instead of drawing from mean over this probability time course, draw the max or the max difference. \n",
    "\n",
    "Seems I also need to do this for DTF over repetitions. Take correlations of these for each subject (rather than max or mean). ANd then plot those correlations. Where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('paper')\n",
    "this_roi = 'V1'\n",
    "subs = np.unique(ALLDM.subj.values) # list of subjects pulled from ALLDM.\n",
    "lookup = dict(zip(['trial_num','run_num','time_point'],['repetition','run','TR']))\n",
    "ivs = ['run_num','trial_num','time_point']\n",
    "this_iv = 'time_point'\n",
    "\n",
    "## do you want to render the CONDITION-wise plots -- trained vs. foil vs control\n",
    "## or the DIFFERENCE plots -- trained - foil vs foil - control?\n",
    "render_cond = 0\n",
    "\n",
    "for this_iv in ivs:\n",
    "    for this_roi in roi_list:\n",
    "\n",
    "        T = []\n",
    "        F = []\n",
    "        C = []\n",
    "        Sub = []\n",
    "        for sub in subs:\n",
    "            inds =(ALLDM['roi']==this_roi) & (ALLDM['subj']==sub) \n",
    "            t,f,c = get_prob_timecourse(this_iv,ALLDM[inds])\n",
    "            if len(T)==0:\n",
    "                T = t\n",
    "                F = f\n",
    "                C = c\n",
    "                DTF = t-f                \n",
    "                DTC = t-c\n",
    "                DFC = f-c\n",
    "            else:\n",
    "                T = np.hstack((T,t))\n",
    "                F = np.hstack((F,f))        \n",
    "                C = np.hstack((C,c)) \n",
    "                DTF = np.hstack((DTF,t-f))                \n",
    "                DTC = np.hstack((DTC,t-c))\n",
    "                DFC = np.hstack((DFC,f-c))\n",
    "            Sub.append([sub]*len(t))   \n",
    "        \n",
    "        if render_cond==1:\n",
    "            ## make longform version of dataframe to use in tsplot (by condition)            \n",
    "            Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "            Condition = np.repeat(['trained','foil','control'],len(T))\n",
    "            Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "            Prob = np.hstack((T,F,C))\n",
    "            assert len(Trial)==len(Condition)\n",
    "            assert len(Sub)==len(Prob)\n",
    "            assert len(Condition)==len(Sub)\n",
    "            x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "            x = x.transpose()\n",
    "            x.columns = ['probability',lookup[this_iv],'condition','sub']\n",
    "            toop = 'condition'\n",
    "        else:\n",
    "            ## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "            Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "            Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "            Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "            Prob = np.hstack((DTF,DTC,DFC))        \n",
    "            assert len(Trial)==len(Condition)\n",
    "            assert len(Sub)==len(Prob)\n",
    "            assert len(Condition)==len(Sub)\n",
    "            x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "            x = x.transpose()\n",
    "            x.columns = ['probability',lookup[this_iv],'condition','sub']        \n",
    "            toop = 'difference'\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        ## plot it\n",
    "        sns.tsplot(data=x,\n",
    "                  time=lookup[this_iv],\n",
    "                  unit='sub',\n",
    "                  condition='condition',\n",
    "                  value='probability',\n",
    "                  ci=95)\n",
    "        if render_cond==1:\n",
    "            plt.ylim(0,2/3)\n",
    "            plt.axhline(1/3,linestyle=':',color='k')  \n",
    "            plt.legend(bbox_to_anchor=(0.8, 1.01))  \n",
    "            plt.title('Classifier evidence by condition in {}'.format(this_roi))\n",
    "            \n",
    "        else:\n",
    "            plt.ylim(-0.3,0.3)\n",
    "            plt.axhline(0,linestyle=':',color='k')  \n",
    "            plt.legend(bbox_to_anchor=(0.7, 1.01))                        \n",
    "            plt.title('Difference in classifier evidence by condition in {}'.format(this_roi))        \n",
    "        plt.xticks(np.arange(np.max(x[lookup[this_iv]].values)+1))\n",
    "        if not os.path.exists('./plots/roi/{}/{}'.format(lookup[this_iv],toop)):\n",
    "            os.makedirs('./plots/roi/{}/{}'.format(lookup[this_iv],toop))\n",
    "        plt.tight_layout()        \n",
    "        plt.savefig('./plots/roi/{}/{}/prob_timecourse_{}_by_{}.pdf'.\\\n",
    "                    format(lookup[this_iv],toop,this_roi,lookup[this_iv]))\n",
    "        plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean...Seems to be calculate of the bottom part of the graph. Look for the mean...There it is! Just swap that for max, and you have it. Though, that's the max difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get subject-level index of contrast between objects during drawing\n",
    "sub_tf = []\n",
    "sub_tc = []\n",
    "sub_fc = []\n",
    "roi = []\n",
    "\n",
    "subs = np.unique(ALLDM.subj.values)\n",
    "ivs = ['time_point'] ## other optoins 'run_num','trial_num',\n",
    "\n",
    "## do you want to render the CONDITION-wise plots -- trained vs. foil vs control\n",
    "## or the DIFFERENCE plots -- trained - foil vs foil - control?\n",
    "render_cond = 0\n",
    "\n",
    "for this_iv in ivs:\n",
    "    for this_roi in roi_list:\n",
    "\n",
    "        T = []\n",
    "        F = []\n",
    "        C = []\n",
    "        Sub = []\n",
    "        for sub in subs:\n",
    "            inds =(ALLDM['roi']==this_roi) & (ALLDM['subj']==sub) \n",
    "            t,f,c = get_prob_timecourse(this_iv,ALLDM[inds])\n",
    "            if len(T)==0:\n",
    "                T = t\n",
    "                F = f\n",
    "                C = c\n",
    "                DTF = t-f                \n",
    "                DTC = t-c\n",
    "                DFC = f-c\n",
    "            else:\n",
    "                T = np.hstack((T,t))\n",
    "                F = np.hstack((F,f))        \n",
    "                C = np.hstack((C,c)) \n",
    "                DTF = np.hstack((DTF,t-f))                \n",
    "                DTC = np.hstack((DTC,t-c))\n",
    "                DFC = np.hstack((DFC,f-c))\n",
    "            Sub.append([sub]*len(t))   \n",
    "          \n",
    "        ## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "        Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "        Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "        Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "        Prob = np.hstack((DTF,DTC,DFC))        \n",
    "        assert len(Trial)==len(Condition)\n",
    "        assert len(Sub)==len(Prob)\n",
    "        assert len(Condition)==len(Sub)\n",
    "        x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "        x = x.transpose()\n",
    "        x.columns = ['probability',lookup[this_iv],'condition','sub']\n",
    "        \n",
    "        for this_sub in subs:\n",
    "            sub_tf.append(x[(x['condition']=='trained-foil') & (x['sub']==this_sub)]['probability'].mean())\n",
    "            sub_tc.append(x[(x['condition']=='trained-control') & (x['sub']==this_sub)]['probability'].mean())  \n",
    "            sub_fc.append(x[(x['condition']=='foil-control') & (x['sub']==this_sub)]['probability'].mean()) \n",
    "            roi.append(this_roi)\n",
    "            \n",
    "## make dataframe with subject-level difference scores\n",
    "d = pd.DataFrame([sub_tf,sub_tc,sub_fc,roi])\n",
    "d = d.transpose()\n",
    "d.columns = ['trained-foil','trained-control','foil-control','roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trained-foil</th>\n",
       "      <th>trained-control</th>\n",
       "      <th>foil-control</th>\n",
       "      <th>roi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0635827</td>\n",
       "      <td>0.0198272</td>\n",
       "      <td>-0.0437555</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.0499401</td>\n",
       "      <td>-0.0336034</td>\n",
       "      <td>0.0163367</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00571547</td>\n",
       "      <td>0.000237982</td>\n",
       "      <td>-0.00547749</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.0342009</td>\n",
       "      <td>0.00021663</td>\n",
       "      <td>0.0344175</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00990037</td>\n",
       "      <td>-0.0180491</td>\n",
       "      <td>-0.0279494</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00523613</td>\n",
       "      <td>0.00523613</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0385715</td>\n",
       "      <td>0.0112056</td>\n",
       "      <td>-0.0273659</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0580379</td>\n",
       "      <td>0.0692755</td>\n",
       "      <td>0.0112376</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0686306</td>\n",
       "      <td>0.0483338</td>\n",
       "      <td>-0.0202969</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.00167627</td>\n",
       "      <td>-0.00167627</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0153465</td>\n",
       "      <td>0.0153465</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0529947</td>\n",
       "      <td>0.0460332</td>\n",
       "      <td>-0.0069615</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0776014</td>\n",
       "      <td>0.0433851</td>\n",
       "      <td>-0.0342164</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.0213628</td>\n",
       "      <td>-0.025072</td>\n",
       "      <td>-0.00370921</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.00573752</td>\n",
       "      <td>0.00326452</td>\n",
       "      <td>0.00900204</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.024833</td>\n",
       "      <td>0.0117156</td>\n",
       "      <td>-0.0131174</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0258537</td>\n",
       "      <td>0.0258537</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0433066</td>\n",
       "      <td>-0.0310209</td>\n",
       "      <td>-0.0743275</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0479191</td>\n",
       "      <td>0.00836561</td>\n",
       "      <td>-0.0395535</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.12695</td>\n",
       "      <td>0.0576521</td>\n",
       "      <td>-0.0692976</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0289629</td>\n",
       "      <td>-0.0289629</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0137961</td>\n",
       "      <td>0.00966771</td>\n",
       "      <td>-0.00412834</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0148355</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>-0.0123665</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.0515594</td>\n",
       "      <td>-0.0191769</td>\n",
       "      <td>0.0323825</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.00564843</td>\n",
       "      <td>0.026622</td>\n",
       "      <td>0.0322705</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00836303</td>\n",
       "      <td>-0.0128755</td>\n",
       "      <td>-0.0212386</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.0334157</td>\n",
       "      <td>-0.0334157</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.0221948</td>\n",
       "      <td>-0.0171668</td>\n",
       "      <td>0.00502793</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0670659</td>\n",
       "      <td>0.0285858</td>\n",
       "      <td>-0.0384801</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0567858</td>\n",
       "      <td>0.0262781</td>\n",
       "      <td>-0.0305077</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0252234</td>\n",
       "      <td>0.0146858</td>\n",
       "      <td>-0.0105375</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trained-foil trained-control foil-control roi\n",
       "0     0.0635827       0.0198272   -0.0437555  V1\n",
       "1    -0.0499401      -0.0336034    0.0163367  V1\n",
       "2    0.00571547     0.000237982  -0.00547749  V1\n",
       "3    -0.0342009      0.00021663    0.0344175  V1\n",
       "4    0.00990037      -0.0180491   -0.0279494  V1\n",
       "5             0      0.00523613   0.00523613  V1\n",
       "6     0.0385715       0.0112056   -0.0273659  V1\n",
       "7     0.0580379       0.0692755    0.0112376  V1\n",
       "8     0.0686306       0.0483338   -0.0202969  V1\n",
       "9             0     -0.00167627  -0.00167627  V1\n",
       "10            0       0.0153465    0.0153465  V1\n",
       "11    0.0529947       0.0460332   -0.0069615  V1\n",
       "12    0.0776014       0.0433851   -0.0342164  V1\n",
       "13   -0.0213628       -0.025072  -0.00370921  V1\n",
       "14  -0.00573752      0.00326452   0.00900204  V1\n",
       "15     0.024833       0.0117156   -0.0131174  V1\n",
       "16            0       0.0258537    0.0258537  V1\n",
       "17    0.0433066      -0.0310209   -0.0743275  V1\n",
       "18    0.0479191      0.00836561   -0.0395535  V1\n",
       "19      0.12695       0.0576521   -0.0692976  V1\n",
       "20            0      -0.0289629   -0.0289629  V1\n",
       "21    0.0137961      0.00966771  -0.00412834  V1\n",
       "22    0.0148355        0.002469   -0.0123665  V1\n",
       "23   -0.0515594      -0.0191769    0.0323825  V1\n",
       "24  -0.00564843        0.026622    0.0322705  V1\n",
       "25   0.00836303      -0.0128755   -0.0212386  V1\n",
       "26            0      -0.0334157   -0.0334157  V1\n",
       "27   -0.0221948      -0.0171668   0.00502793  V1\n",
       "28    0.0670659       0.0285858   -0.0384801  V1\n",
       "29    0.0567858       0.0262781   -0.0305077  V1\n",
       "30    0.0252234       0.0146858   -0.0105375  V1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['roi']=='V1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just tests if any of these differences are reliably different from zero. Some are, even taking into account multiple measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = 2.768736123485365, p = 0.009554778830532455\n",
      "ROI = V2 | t = 2.770583015369253, p = 0.0095119210686561\n",
      "ROI = LOC | t = 4.596186237182268, p = 7.268034217384911e-05\n",
      "ROI = IT | t = 0.32746038433578734, p = 0.7455939050323976\n",
      "ROI = fusiform | t = 2.1010699856853625, p = 0.04414153968778241\n",
      "ROI = parahippo | t = 0.5293455190029417, p = 0.6004613045029474\n",
      "ROI = PRC | t = 0.016453483562522103, p = 0.986981551664948\n",
      "ROI = ento | t = -0.6840172702888475, p = 0.49921431079537637\n",
      "ROI = hipp | t = -1.0407359788537531, p = 0.3063133092292496\n",
      "ROI = mOFC | t = 0.5196500057946879, p = 0.6071208489160427\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['trained-foil']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = 1.6636248275724903, p = 0.10660079640244327\n",
      "ROI = V2 | t = 2.2139350332488115, p = 0.03457771253468806\n",
      "ROI = LOC | t = 2.415038847515531, p = 0.02203647713504597\n",
      "ROI = IT | t = -0.3861479810294371, p = 0.7021121715021897\n",
      "ROI = fusiform | t = 1.6227296794866946, p = 0.11511072025005005\n",
      "ROI = parahippo | t = -0.27083607057368286, p = 0.7883718940652598\n",
      "ROI = PRC | t = 1.2985961595839934, p = 0.20397641602298494\n",
      "ROI = ento | t = 0.6538942794524435, p = 0.5181612790511406\n",
      "ROI = hipp | t = -0.4834968962225369, p = 0.6322533131124466\n",
      "ROI = mOFC | t = 0.4752748443740968, p = 0.6380333575379928\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['trained-control']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI = V1 | t = -2.3466838298413037, p = 0.0257378940389738\n",
      "ROI = V2 | t = -2.5411352786501786, p = 0.016457522733690673\n",
      "ROI = LOC | t = -3.1123034695161897, p = 0.004055278623925445\n",
      "ROI = IT | t = -0.8491410233102968, p = 0.4025281934473953\n",
      "ROI = fusiform | t = -1.4784845160787863, p = 0.14970188244735996\n",
      "ROI = parahippo | t = -0.9710093202551024, p = 0.33930791332804766\n",
      "ROI = PRC | t = 0.9153607662983471, p = 0.36730173339353445\n",
      "ROI = ento | t = 1.3457073383157303, p = 0.1884798586934702\n",
      "ROI = hipp | t = 0.8915989428781647, p = 0.37970360289586\n",
      "ROI = mOFC | t = 0.1614653384488601, p = 0.8728094242188358\n"
     ]
    }
   ],
   "source": [
    "for this_roi in roi_list:\n",
    "    data = d[d['roi']==this_roi]['foil-control']\n",
    "    t,p = stats.ttest_1samp(data,0)\n",
    "    print ('ROI = {} | t = {}, p = {}'.format(this_roi,t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple graph. X is foil-control, y is prepostdiff scores. Plot points, correlation, etc. All I'm changing is x-axis, and I'm doing it above. Cool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepost = pd.read_csv('neural_changes_by_surfroi_and_subject.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1\n",
      "V2\n",
      "LOC\n",
      "IT\n",
      "fusiform\n",
      "parahippo\n",
      "PRC\n",
      "ento\n",
      "hipp\n",
      "mOFC\n"
     ]
    }
   ],
   "source": [
    "## make dataframe to relate drawing contrast to recognition differentiation\n",
    "roi_list = ['V1', 'V2', 'LOC', 'IT', 'fusiform', 'parahippo', 'PRC', 'ento','hipp', 'mOFC']\n",
    "this_roi = 'hipp'\n",
    "\n",
    "for this_roi in roi_list:\n",
    "    print(this_roi)\n",
    "    draw = d[d['roi']==this_roi]['trained-control'].values - d[d['roi']==this_roi]['foil-control'].values\n",
    "    recog = prepost['tradiff_{}'.format(this_roi)].values-prepost['condiff_{}'.format(this_roi)].values\n",
    "\n",
    "    z = pd.DataFrame([draw,recog])\n",
    "    z = z.transpose()\n",
    "    z.columns=['draw','recog']\n",
    "\n",
    "    ## plot \n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    sns.set_context('poster')\n",
    "    sns.regplot(x=\"draw\",\n",
    "                y =\"recog\",\n",
    "                data=z)\n",
    "    r,p = stats.pearsonr(draw,recog)\n",
    "    plt.title('ROI: {}  r={}  p={}'.format(this_roi,np.round(r,3),np.round(p,3)))\n",
    "    if not os.path.exists('./plots/roi/drawrecog'):\n",
    "        os.makedirs('./plots/roi/drawrecog')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/roi/drawrecog/draw_recog_scatter_{}.pdf'.format(this_roi))\n",
    "    plt.close(fig)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relate neural to vgg drawing time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./logistic_timeseries_drawing_vgg.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-171bafc1696e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logistic_timeseries_drawing_vgg.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneural_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logistic_timeseries_drawing_neural.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jgunn/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./logistic_timeseries_drawing_vgg.csv' does not exist"
     ]
    }
   ],
   "source": [
    "vgg_ts = pd.read_csv('./logistic_timeseries_drawing_vgg.csv')\n",
    "neural_ts = pd.read_csv('./logistic_timeseries_drawing_neural.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_ts = vgg_ts.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)\n",
    "vgg_ts.wID = [i.split('_')[0] for i in vgg_ts.wID.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>wID</th>\n",
       "      <th>viewpoint</th>\n",
       "      <th>trial</th>\n",
       "      <th>trialDuration</th>\n",
       "      <th>target</th>\n",
       "      <th>competitor</th>\n",
       "      <th>numSketch</th>\n",
       "      <th>bed</th>\n",
       "      <th>bench</th>\n",
       "      <th>chair</th>\n",
       "      <th>table</th>\n",
       "      <th>curr_winner</th>\n",
       "      <th>tc_pair</th>\n",
       "      <th>trialID</th>\n",
       "      <th>run</th>\n",
       "      <th>target_val</th>\n",
       "      <th>competitor_val</th>\n",
       "      <th>control_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>chair</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>0.013931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934225</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>bed</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.467345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.970770</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970770</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.013258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.008644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0119174</td>\n",
       "      <td>20</td>\n",
       "      <td>320</td>\n",
       "      <td>39.00144</td>\n",
       "      <td>bench</td>\n",
       "      <td>chair</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019298</td>\n",
       "      <td>0.967674</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>bench</td>\n",
       "      <td>bench/chair</td>\n",
       "      <td>0119174_neurosketch_320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967674</td>\n",
       "      <td>0.001463</td>\n",
       "      <td>0.015431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      wID  viewpoint  trial  trialDuration target competitor  \\\n",
       "0      0  0119174         20    320       39.00144  bench      chair   \n",
       "1      0  0119174         20    320       39.00144  bench      chair   \n",
       "2      0  0119174         20    320       39.00144  bench      chair   \n",
       "3      0  0119174         20    320       39.00144  bench      chair   \n",
       "4      0  0119174         20    320       39.00144  bench      chair   \n",
       "\n",
       "   numSketch       bed     bench     chair     table curr_winner      tc_pair  \\\n",
       "0          0  0.026822  0.055895  0.916243  0.001040       chair  bench/chair   \n",
       "1          1  0.934225  0.026225  0.039085  0.000465         bed  bench/chair   \n",
       "2          2  0.006035  0.970770  0.002714  0.020481       bench  bench/chair   \n",
       "3          3  0.009842  0.978927  0.003784  0.007447       bench  bench/chair   \n",
       "4          4  0.019298  0.967674  0.001463  0.011564       bench  bench/chair   \n",
       "\n",
       "                   trialID  run  target_val  competitor_val  control_val  \n",
       "0  0119174_neurosketch_320  1.0    0.055895        0.916243     0.013931  \n",
       "1  0119174_neurosketch_320  1.0    0.026225        0.039085     0.467345  \n",
       "2  0119174_neurosketch_320  1.0    0.970770        0.002714     0.013258  \n",
       "3  0119174_neurosketch_320  1.0    0.978927        0.003784     0.008644  \n",
       "4  0119174_neurosketch_320  1.0    0.967674        0.001463     0.015431  "
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2000489150>]"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEECAYAAAAlEzNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZE0pyAQEUlzCgIRkTSnIBARSXP/P9WPn1x+dViFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ff3e34f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vgg_ts.groupby('numSketch')['target_val'].mean())\n",
    "plt.plot(vgg_ts.groupby('numSketch')['competitor_val'].mean(),color='green')\n",
    "plt.plot(vgg_ts.groupby('numSketch')['control_val'].mean(),color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plotting helper\n",
    "def get_vgg_timecourse(iv,DM):\n",
    "    trained_objs = np.unique(DM.target.values)\n",
    "    control_objs = [i for i in ['bed','bench','chair','table'] if i not in trained_objs]\n",
    "    t1 = trained_objs[0]\n",
    "    t2 = trained_objs[1]\n",
    "    c1 = control_objs[0]\n",
    "    c2 = control_objs[1]\n",
    "    target = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t1)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t2)].mean().values)).mean(0) ## target timecourse\n",
    "    foil = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(t2)].mean().values,\n",
    "                   DM[DM.label==t2].groupby(iv)['{}_prob'.format(t1)].mean().values)).mean(0) ## foil timecourse\n",
    "    control = np.vstack((DM[DM.label==t1].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t1].groupby(iv)['{}_prob'.format(c2)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c1)].mean().values,\n",
    "                        DM[DM.label==t2].groupby(iv)['{}_prob'.format(c2)].mean().values)).mean(0) ## control timecourse\n",
    "    \n",
    "    return target, foil, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = []\n",
    "F = []\n",
    "C = []\n",
    "Sub = []\n",
    "for sub in subs:\n",
    "    inds =(vgg_ts['wID']==sub) \n",
    "    t,f,c = get_prob_timecourse(this_iv,vgg_ts[inds])\n",
    "    \n",
    "    \n",
    "    if len(T)==0:\n",
    "        T = t\n",
    "        F = f\n",
    "        C = c\n",
    "        DTF = t-f                \n",
    "        DTC = t-c\n",
    "        DFC = f-c\n",
    "    else:\n",
    "        T = np.hstack((T,t))\n",
    "        F = np.hstack((F,f))        \n",
    "        C = np.hstack((C,c)) \n",
    "        DTF = np.hstack((DTF,t-f))                \n",
    "        DTC = np.hstack((DTC,t-c))\n",
    "        DFC = np.hstack((DFC,f-c))\n",
    "    Sub.append([sub]*len(t))   \n",
    "\n",
    "## make longform version of dataframe to use in tsplot (difference btw conditions)                    \n",
    "Trial = np.tile(np.arange(len(t)),len(subs)*3)\n",
    "Condition = np.repeat(['trained-foil','trained-control','foil-control'],len(T))\n",
    "Sub = np.tile(np.array(flatten(Sub)),3)\n",
    "Prob = np.hstack((DTF,DTC,DFC))        \n",
    "assert len(Trial)==len(Condition)\n",
    "assert len(Sub)==len(Prob)\n",
    "assert len(Condition)==len(Sub)\n",
    "x = pd.DataFrame([Prob,Trial,Condition,Sub])\n",
    "x = x.transpose()\n",
    "x.columns = ['probability',lookup[this_iv],'condition','sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0110171', '0110172', '0111171', '0112171', '0112172', '0112173',\n",
       "       '0113171', '0115174', '0117171', '0118171', '0118172', '0119171',\n",
       "       '0119172', '0119173', '0119174', '0120171', '0120172', '0120173',\n",
       "       '0123171', '0123173', '0124171', '0125171', '0125172', '1121161',\n",
       "       '1130161', '1202161', '1203161', '1206161', '1206162', '1206163',\n",
       "       '1207162'], dtype=object)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0110171_neurosketch', '0110172_neurosketch', '0111171_neurosketch', '0112171_neurosketch', '0112172_neurosketch', '0112173_neurosketch', '0113171_neurosketch', '0115174_neurosketch', '0117171_neurosketch', '0118171_neurosketch', '0118172_neurosketch', '0119171_neurosketch', '0119172_neurosketch', '0119173_neurosketch', '0119174_neurosketch', '0120171_neurosketch', '0120172_neurosketch', '0120173_neurosketch', '0123171_neurosketch', '0123173_neurosketch', '0124171_neurosketch', '0125171_neurosketch', '0125172_neurosketch', '1121161_neurosketch', '1130161_neurosketch', '1202161_neurosketch', '1203161_neurosketch', '1206161_neurosketch', '1206162_neurosketch', '1206163_neurosketch', '1207162_neurosketch']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(['1121161_neurosketch', '1130161_neurosketch', '1202161_neurosketch', '1203161_neurosketch', '1206161_neurosketch', '1206162_neurosketch', '1206163_neurosketch', '1207162_neurosketch', '0110171_neurosketch', \n",
    "                '0110172_neurosketch', '0111171_neurosketch', '0112171_neurosketch', '0112172_neurosketch', '0112173_neurosketch', '0113171_neurosketch', '0115174_neurosketch', '0117171_neurosketch', '0118171_neurosketch', \n",
    "                '0118172_neurosketch', '0119171_neurosketch', '0119172_neurosketch', '0119173_neurosketch', '0119174_neurosketch', '0120171_neurosketch', '0120172_neurosketch', '0120173_neurosketch', '0123171_neurosketch', \n",
    "                '0123173_neurosketch', '0124171_neurosketch', '0125171_neurosketch', '0125172_neurosketch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
